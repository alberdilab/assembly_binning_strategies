[["index.html", "AlberdiLab | Eisenhofer et al. in prep Contrasting assembly and binning strategies recover different genome catalogues 1 Introduction 1.1 Prepare the R environment", " AlberdiLab | Eisenhofer et al. in prep Contrasting assembly and binning strategies recover different genome catalogues Raphael Eisenhofer1, Antton Alberdi2, Ostaizka Aizpurua3 Latest update: 2024-05-26 1 Introduction This webbook contains all the code used for data analysis in study of the population-level metagenomic data of Podarcis muralis lizards across elevational gradients in various mountain ranges of the Pyrenees. 1.1 Prepare the R environment 1.1.1 Environment To reproduce all the analyses locally, clone this repository in your computer using: RStudio &gt; New Project &gt; Version Control &gt; Git And indicating the following git repository: https://github.com/alberdilab/assembly_binning_strategies.git Once the R project has been created, follow the instructions and code chunks shown in this webbook. 1.1.2 Libraries The following R packages are required for the data analysis. # Base library(R.utils) library(knitr) library(tidyverse) library(devtools) library(tinytable) library(rairtable) # For tree handling library(ape) library(phyloseq) library(phytools) # For plotting library(ggplot2) library(ggrepel) library(ggpubr) library(ggnewscale) library(gridExtra) library(ggtreeExtra) library(ggtree) library(ggh4x) library(patchwork) # For statistics library(spaa) library(vegan) library(Rtsne) library(geiger) library(hilldiv2) library(distillR) library(broom.mixed) #library(lmerTest) library(Hmsc) library(corrplot) University of Copenhagen, raphael.eisenhofer@sund.ku.dk↩︎ University of Copenhagen, antton.alberdi@sund.ku.dk↩︎ University of Copenhagen, ostaizka.aizpurua@sund.ku.dk↩︎ "],["data-preparation.html", "2 Data preparation 2.1 Metadata 2.2 Read counts 2.3 Cluster taxonomy 2.4 Cluster tree 2.5 Cluster counts 2.6 Cluster functions 2.7 Color scheme 2.8 Wrap working objects", " 2 Data preparation 2.1 Metadata bin_metadata &lt;- read_delim(&quot;data/bin_metadata.tsv.gz&quot;) %&gt;% rename(genome=bin_id) %&gt;% mutate(comp_cont = completeness - contamination) %&gt;% mutate(overall_strategy = case_when( overall_strategy == &quot;individual_individual_binning&quot; ~ &quot;single_coverage&quot;, overall_strategy == &quot;cobinning_longitudinal&quot; ~ &quot;multicoverage_animal&quot;, overall_strategy == &quot;cobinning_treatment&quot; ~ &quot;multicoverage_timepoint_all&quot;, overall_strategy == &quot;cobinning_cage_treatment&quot; ~ &quot;multicoverage_timepoint_cage&quot;, overall_strategy == &quot;coassembly_longitudinal&quot; ~ &quot;coassembly_animal&quot;, overall_strategy == &quot;coassembly_treatment&quot; ~ &quot;coassembly_timepoint_all&quot;, overall_strategy == &quot;coassembly_cage_treatment&quot; ~ &quot;coassembly_timepoint_cage&quot;, overall_strategy == &quot;multi_split_longitudinal&quot; ~ &quot;multisplit_animal&quot;, overall_strategy == &quot;multi_split_treatment&quot; ~ &quot;multisplit_timepoint_all&quot;, overall_strategy == &quot;multi_split_cage_treatment&quot; ~ &quot;multisplit_timepoint_cage&quot;, TRUE ~ overall_strategy)) %&gt;% mutate(assembly = case_when( assembly == &quot;individual&quot; ~ &quot;single_coverage&quot;, assembly == &quot;cobinning&quot; ~ &quot;multi_coverage&quot;, assembly == &quot;coassembly&quot; ~ &quot;coassembly&quot;, assembly == &quot;multi_split&quot; ~ &quot;multi_split&quot;, TRUE ~ assembly)) %&gt;% mutate(strategy = case_when( strategy == &quot;individual_binning&quot; ~ &quot;NA&quot;, strategy == &quot;longitudinal&quot; ~ &quot;animal&quot;, strategy == &quot;treatment&quot; ~ &quot;timepoint_all&quot;, strategy == &quot;cage_treatment&quot; ~ &quot;timepoint_cage&quot;, TRUE ~ strategy)) %&gt;% mutate(genome = str_replace(genome, &quot;individual&quot;, &quot;single_coverage&quot;), genome = str_replace(genome, &quot;cobinning_long&quot;, &quot;multicoverage_animal&quot;), genome = str_replace(genome, &quot;cobinning_treat&quot;, &quot;multicoverage_timepoint_all&quot;), genome = str_replace(genome, &quot;cobinning_cage_treat&quot;, &quot;multicoverage_timepoint_cage&quot;), genome = str_replace(genome, &quot;coassembly_long&quot;, &quot;coassembly_animal&quot;), genome = str_replace(genome, &quot;coassembly_treat&quot;, &quot;coassembly_timepoint_all&quot;), genome = str_replace(genome, &quot;coassembly_cage_treat&quot;, &quot;coassembly_timepoint_cage&quot;), genome = str_replace(genome, &quot;vamb_long&quot;, &quot;multisplit_animal&quot;), genome = str_replace(genome, &quot;vamb_treat&quot;, &quot;multisplit_timepoint_all&quot;), genome = str_replace(genome, &quot;vamb_cage_treat&quot;, &quot;multisplit_timepoint_cage&quot;)) assemblies &lt;- c(&quot;single_coverage&quot;, &quot;multi_coverage&quot;, &quot;coassembly&quot;, &quot;multi_split&quot;) strategies &lt;- c(&quot;single_coverage&quot;, &quot;multicoverage_animal&quot;, &quot;multicoverage_timepoint_all&quot;, &quot;multicoverage_timepoint_cage&quot;, &quot;coassembly_animal&quot;, &quot;coassembly_timepoint_all&quot;, &quot;coassembly_timepoint_cage&quot;, &quot;multisplit_animal&quot;, &quot;multisplit_timepoint_all&quot;, &quot;multisplit_timepoint_cage&quot;) 2.2 Read counts import_count_data &lt;- function(strategy) { read_delim(file = paste0(&#39;data/count_tables/count_table_&#39;,strategy,&#39;.tsv.gz&#39;)) %&gt;% rename(genome=Genome) %&gt;% filter(genome != &quot;unmapped&quot;) %&gt;% pivot_longer(!genome, names_to = c(&quot;sample&quot;, &quot;.value&quot;), names_sep = &quot; &quot;) %&gt;% rename(&quot;read_count&quot; = &quot;Read&quot;) %&gt;% rename(&quot;covered_fraction&quot; = &quot;Covered&quot;) %&gt;% rename(&quot;MAG_length&quot; = &quot;Length&quot;) %&gt;% mutate(genome = str_replace_all(genome, &quot;^&quot;, paste0(strategy, &quot;_&quot;)), genome = str_replace_all(genome, &quot;_OP_OP&quot;, &quot;_OP&quot;), genome = str_replace_all(genome, &quot;vamb_long_C.*_C&quot;, &quot;vamb_long_C&quot;), sample = str_remove(sample, &quot;_subsam_M&quot;), sample = str_remove(sample, &quot;_subsam&quot;), sample = str_replace(sample, &quot;CD&quot;,&quot;CT&quot;)) %&gt;% left_join(bin_metadata, by = &quot;genome&quot;) %&gt;% select(genome, sample, read_count, covered_fraction, MAG_length, primary_cluster, secondary_cluster, overall_strategy, assembly, strategy) } bin_counts &lt;- purrr::map(strategies,import_count_data) %&gt;% list_rbind() 2.3 Cluster taxonomy cluster_taxonomy &lt;- read_delim(&quot;data/cluster_taxonomy.tsv&quot;) 2.4 Cluster tree cluster_tree &lt;- read_tree(str_glue(&quot;data/cluster_tree.tre&quot;)) 2.4.1 Normalise read counts Normalise read counts into genome counts. bin_counts_norm &lt;- bin_counts %&gt;% filter(secondary_cluster %in% cluster_tree$tip.label) %&gt;% mutate(read_count = read_count*150/MAG_length) 2.4.2 Filter read counts Filter out genome counts with coverage fraction under 30%. bin_counts_norm_filt &lt;- bin_counts_norm %&gt;% mutate(read_count = ifelse(covered_fraction&gt;=0.3,read_count,0)) 2.5 Cluster counts Aggregate bins into clusters. cluster_counts &lt;- bin_counts_norm_filt %&gt;% group_by(sample,overall_strategy,secondary_cluster) %&gt;% summarise(read_count=sum(read_count), .groups=&quot;drop&quot;) %&gt;% filter(!is.na(overall_strategy)) %&gt;% select(secondary_cluster,overall_strategy,read_count) 2.6 Cluster functions cluster_kegg &lt;- read_delim(&quot;data/cluster_kegg.tsv&quot;) %&gt;% select(-genome) %&gt;% group_by(secondary_cluster,overall_strategy) %&gt;% summarise(across(where(is.numeric), max)) %&gt;% mutate(overall_strategy = case_when( overall_strategy == &quot;individual_individual_binning&quot; ~ &quot;single_coverage&quot;, overall_strategy == &quot;cobinning_longitudinal&quot; ~ &quot;multicoverage_animal&quot;, overall_strategy == &quot;cobinning_treatment&quot; ~ &quot;multicoverage_timepoint_all&quot;, overall_strategy == &quot;cobinning_cage_treatment&quot; ~ &quot;multicoverage_timepoint_cage&quot;, overall_strategy == &quot;coassembly_longitudinal&quot; ~ &quot;coassembly_animal&quot;, overall_strategy == &quot;coassembly_treatment&quot; ~ &quot;coassembly_timepoint_all&quot;, overall_strategy == &quot;coassembly_cage_treatment&quot; ~ &quot;coassembly_timepoint_cage&quot;, overall_strategy == &quot;multi_split_longitudinal&quot; ~ &quot;multisplit_animal&quot;, overall_strategy == &quot;multi_split_treatment&quot; ~ &quot;multisplit_timepoint_all&quot;, overall_strategy == &quot;multi_split_cage_treatment&quot; ~ &quot;multisplit_timepoint_cage&quot;, TRUE ~ overall_strategy)) 2.7 Color scheme AlberdiLab projects use unified color schemes developed for the Earth Hologenome Initiative, to facilitate figure interpretation. phylum_colors &lt;- cluster_taxonomy %&gt;% left_join(read_tsv(&quot;https://raw.githubusercontent.com/earthhologenome/EHI_taxonomy_colour/main/ehi_phylum_colors.tsv&quot;) %&gt;% mutate(phylum=gsub(&quot;p__&quot;,&quot;&quot;,phylum)), by=join_by(Phylum == phylum)) %&gt;% select(Phylum, colors) %&gt;% unique() %&gt;% arrange(Phylum) %&gt;% select(colors) %&gt;% pull() strategy_colors &lt;- c(single_coverage=&quot;#5254A3&quot;, multicoverage_animal=&quot;#637939&quot;, multicoverage_timepoint_all=&quot;#8CA252&quot;, multicoverage_timepoint_cage=&quot;#B5CF6B&quot;, coassembly_animal=&quot;#8C6D31&quot;, coassembly_timepoint_all=&quot;#BD9E39&quot;, coassembly_timepoint_cage=&quot;#E7BA52&quot;, multisplit_animal=&quot;#843C39&quot;, multisplit_timepoint_all=&quot;#AD494A&quot;, multisplit_timepoint_cage=&quot;#D6616B&quot;) assembly_colors &lt;- c(single_coverage=&quot;#5254A3&quot;, multi_coverage=&quot;#8CA252&quot;, coassembly=&quot;#BD9E39&quot;, multi_split=&quot;#AD494A&quot;) 2.8 Wrap working objects In the last step, the objects that are needed for downstream analyses are stored in an R object. save(bin_metadata, strategies, bin_counts, bin_counts_norm, bin_counts_norm_filt, cluster_counts, cluster_taxonomy, cluster_tree, cluster_kegg, phylum_colors, strategy_colors, assembly_colors, file = &quot;data/data.Rdata&quot;) "],["cluster-analysis.html", "3 Cluster analysis 3.1 Cluster prevalence 3.2 Cluster frequency 3.3 Cluster heatmap 3.4 Cluster abundance 3.5 MAG mapping 3.6 Domain-adjusted mapping rates (DAMR) 3.7 Read counts vs prevalence", " 3 Cluster analysis 3.1 Cluster prevalence Number of strategies each cluster was captured in. cluster_prevalence &lt;- cluster_counts %&gt;% filter(read_count&gt;0) %&gt;% group_by(secondary_cluster) %&gt;% summarise(n_strategy = n_distinct(overall_strategy)) %&gt;% arrange(desc(n_strategy)) #Arranged by phylum and cluster prevalence cluster_prevalence_phylum &lt;- cluster_prevalence %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) 3.2 Cluster frequency Number of clusters each strategy recovered. cluster_number &lt;- cluster_counts %&gt;% filter(read_count&gt;0) %&gt;% group_by(overall_strategy) %&gt;% summarise(n = n_distinct(secondary_cluster)) %&gt;% arrange(desc(n)) %&gt;% mutate(overall_strategy=factor(overall_strategy,levels=rev(overall_strategy))) cluster_number %&gt;% rename(Strategy=overall_strategy) %&gt;% select(Strategy,n) %&gt;% tt() tinytable_6nr2caxhuvqwyavjssqo .table td.tinytable_css_brzi364d1d0pepxf1mjo, .table th.tinytable_css_brzi364d1d0pepxf1mjo { border-bottom: solid 0.1em #d3d8dc; } Strategy n coassembly_timepoint_all 176 coassembly_animal 163 coassembly_timepoint_cage 160 multicoverage_animal 130 multicoverage_timepoint_cage 126 single_coverage 117 multisplit_timepoint_all 115 multisplit_timepoint_cage 110 multisplit_animal 99 multicoverage_timepoint_all 93 cluster_number %&gt;% filter(overall_strategy != &quot;cobinning_treatment&quot;) %&gt;% ggplot(aes(x = n, y = overall_strategy)) + geom_col(color=&quot;#666666&quot;) + coord_cartesian(xlim = c(90,200))+ theme_classic() + theme() + labs(x=&quot;Number of clusters&quot;, y=&quot;Strategy&quot;) 3.3 Cluster heatmap Overview of clusters per strategy. cluster_counts %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) %&gt;% mutate(Phylum=ifelse(is.na(Phylum),&quot;Bacillota_A&quot;,Phylum)) %&gt;% mutate(secondary_cluster=factor(secondary_cluster,levels=rev(cluster_tree$tip.label))) %&gt;% mutate(overall_strategy=factor(overall_strategy,levels=rev(cluster_number$overall_strategy))) %&gt;% group_by(overall_strategy, secondary_cluster) %&gt;% sample_n(1) %&gt;% ungroup() %&gt;% ggplot(aes(x = secondary_cluster, y = overall_strategy, fill=Phylum)) + geom_tile(color=&quot;#ffffff&quot;) + scale_fill_manual(values=phylum_colors) + theme_classic() + theme(axis.text.x = element_blank(), axis.title.y = element_blank()) + xlab(&quot;MAG clusters&quot;) 3.4 Cluster abundance Maximum read count maximum_read_count &lt;- cluster_counts %&gt;% group_by(secondary_cluster) %&gt;% slice_max(order_by = read_count) %&gt;% ungroup() maximum_read_count %&gt;% mutate(secondary_cluster=factor(secondary_cluster,levels=rev(cluster_tree$tip.label))) %&gt;% ggplot(aes(x = secondary_cluster, y = read_count)) + geom_col(color=&quot;#666666&quot;) + theme_classic() + theme(axis.text.x = element_blank()) + labs(x=&quot;MAG clusters&quot;, y=&quot;Number of reads&quot;) 3.5 MAG mapping Average percentage of read mapped in each strategy against the respective catalogue. all_counts &lt;- read_delim(&quot;data/all_counts.tsv&quot;, col_names = c(&quot;sample&quot;, &quot;total_count&quot;)) %&gt;% mutate(total_count = total_count * 2) mag_mapping &lt;- bin_counts %&gt;% left_join(all_counts, by = join_by(sample)) %&gt;% summarise(mag_mapping = sum(read_count) / mean(total_count), .by = c(sample, overall_strategy)) %&gt;% summarise(mag_mapping = mean(mag_mapping), .by = overall_strategy) %&gt;% mutate(overall_strategy=factor(overall_strategy,levels=rev(cluster_number$overall_strategy))) mag_mapping %&gt;% filter(!is.na(overall_strategy)) %&gt;% rename(Strategy=overall_strategy,`Mapping %`=mag_mapping) %&gt;% mutate(`Mapping %`=`Mapping %`*100) %&gt;% tt() tinytable_whdy7s5s8q7xo4pet01u .table td.tinytable_css_wichabh0rb4xsc8fqipb, .table th.tinytable_css_wichabh0rb4xsc8fqipb { border-bottom: solid 0.1em #d3d8dc; } Strategy Mapping % single_coverage 80.25097 multicoverage_animal 79.78157 multicoverage_timepoint_all 79.68070 multicoverage_timepoint_cage 80.48701 coassembly_animal 80.91453 coassembly_timepoint_all 82.53138 coassembly_timepoint_cage 80.11122 multisplit_animal 68.33532 multisplit_timepoint_all 83.70626 multisplit_timepoint_cage 77.11125 mag_mapping %&gt;% filter(overall_strategy != &quot;cobinning_treatment&quot;) %&gt;% ggplot(aes(x = mag_mapping, y = overall_strategy)) + geom_col(color=&quot;#666666&quot;) + coord_cartesian(xlim = c(0.6,0.9))+ theme_classic() + theme() + labs(x=&quot;Mapping percentage&quot;, y=&quot;Strategy&quot;) 3.6 Domain-adjusted mapping rates (DAMR) #import SingleM estimates smf_files &lt;- list.files(path = &quot;data/singlem/&quot;, pattern = &quot;*tsv.gz&quot;, full.names = T) smf_estimates &lt;- purrr::map(smf_files, read_delim) %&gt;% bind_rows() mean_smf &lt;- smf_estimates %&gt;% mutate(read_fraction = as.numeric(str_replace(read_fraction, &quot;%&quot;, &quot;&quot;))) %&gt;% pull(read_fraction) %&gt;% mean() mag_mapping %&gt;% filter(!is.na(overall_strategy)) %&gt;% rename(Strategy=overall_strategy) %&gt;% mutate(mag_mapping = mag_mapping * 100, DAMR = (mag_mapping / mean_smf) * 100, DAMR = scales::number(if_else(DAMR &gt; 100, 100, DAMR), accuracy = 0.1) ) %&gt;% select(-mag_mapping) %&gt;% tt() tinytable_5vzqzp7v6acalzxuagov .table td.tinytable_css_nqbqbkyrii0rj771lyt6, .table th.tinytable_css_nqbqbkyrii0rj771lyt6 { border-bottom: solid 0.1em #d3d8dc; } Strategy DAMR single_coverage 100.0 multicoverage_animal 100.0 multicoverage_timepoint_all 100.0 multicoverage_timepoint_cage 100.0 coassembly_animal 100.0 coassembly_timepoint_all 100.0 coassembly_timepoint_cage 100.0 multisplit_animal 87.9 multisplit_timepoint_all 100.0 multisplit_timepoint_cage 99.2 3.7 Read counts vs prevalence maximum_read_count %&gt;% left_join(cluster_prevalence,by=join_by(secondary_cluster==secondary_cluster)) %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) %&gt;% select(read_count, n_strategy, Phylum) %&gt;% ggplot(aes(x = read_count, y = n_strategy, group=n_strategy, color=Phylum)) + geom_boxplot(color=&quot;#999999&quot;, fill=&quot;#f4f4f4&quot;, outlier.shape = NA) + scale_y_continuous(breaks=seq(1,10,1))+ scale_color_manual(values=phylum_colors) + geom_jitter() + theme_classic() + theme() + labs(x=&quot;Sequencing depth&quot;, y=&quot;Strategy prevalence&quot;) "],["bin-quality.html", "4 Bin quality", " 4 Bin quality load(&quot;data/data.Rdata&quot;) #Generate quality biplot bin_metadata %&gt;% select(genome,completeness,contamination,overall_strategy, assembly, size) %&gt;% ggplot(aes(x=completeness,y=contamination,size=size,color=overall_strategy)) + geom_point(alpha=0.3, size=1.5) + scale_color_manual(values=strategy_colors)+ ylim(c(10,0)) + labs(y= &quot;Contamination&quot;, x = &quot;Completeness&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) bin_metadata %&gt;% select(genome,completeness,contamination,overall_strategy, assembly, size) %&gt;% mutate(assembly=factor(assembly,levels=assemblies)) %&gt;% ggplot(aes(y=completeness,x=overall_strategy, group=overall_strategy, color=overall_strategy)) + geom_jitter(alpha=0.1)+ geom_boxplot(outlier.shape = NA)+ scale_color_manual(values=strategy_colors)+ facet_grid(~assembly, space = &quot;free&quot;, scale=&quot;free&quot;)+ labs(x = &quot;Strategies&quot;, y=&quot;Completeness&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) bin_metadata %&gt;% select(genome,completeness,contamination,overall_strategy, assembly, size) %&gt;% ggplot(aes(x=completeness,color=assembly)) + geom_density(alpha=0.3, size=1.5, linewidth=1) + scale_color_manual(values=assembly_colors)+ labs(x = &quot;Completeness&quot;, y=&quot;Density&quot;) + theme_classic() bin_metadata %&gt;% select(genome,completeness,contamination,overall_strategy, assembly, size) %&gt;% mutate(assembly=factor(assembly,levels=assemblies)) %&gt;% ggplot(aes(y=contamination,x=overall_strategy, group=overall_strategy, color=overall_strategy)) + geom_jitter(alpha=0.1)+ geom_boxplot(outlier.shape = NA)+ scale_color_manual(values=strategy_colors)+ facet_grid(~assembly, space = &quot;free&quot;, scale=&quot;free&quot;)+ labs(x = &quot;Strategies&quot;, y=&quot;Contamination&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) bin_metadata %&gt;% select(genome,completeness,contamination,assembly, size) %&gt;% ggplot(aes(x=contamination,color=assembly,)) + geom_density(alpha=0.3, size=1.5, linewidth=1) + scale_color_manual(values=assembly_colors)+ labs(x = &quot;Contamination&quot;, y=&quot;Density&quot;) + theme_classic() "],["functional-traits.html", "5 Functional traits 5.1 Trait overview 5.2 Trait recovery", " 5 Functional traits load(&quot;data/data.Rdata&quot;) cluster_kegg_max &lt;- cluster_kegg %&gt;% filter(!is.na(secondary_cluster)) %&gt;% select(-overall_strategy) %&gt;% group_by(secondary_cluster) %&gt;% summarise(across(everything(), max, na.rm = TRUE)) 5.1 Trait overview phylum_heatmap &lt;- read_tsv(&quot;https://raw.githubusercontent.com/earthhologenome/EHI_taxonomy_colour/main/ehi_phylum_colors.tsv&quot;) %&gt;% right_join(cluster_taxonomy, by=join_by(phylum == Phylum)) %&gt;% filter(secondary_cluster %in% cluster_tree$tip.label) %&gt;% arrange(match(secondary_cluster, cluster_tree$tip.label)) %&gt;% select(secondary_cluster,phylum) %&gt;% mutate(phylum = factor(phylum, levels = unique(phylum))) %&gt;% filter(!is.na(secondary_cluster)) %&gt;% column_to_rownames(var = &quot;secondary_cluster&quot;) function_tree &lt;- keep.tip(cluster_tree, tip=rownames(phylum_heatmap)) function_table &lt;- cluster_kegg_max %&gt;% filter(secondary_cluster %in% function_tree$tip.label) %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;) # Generate basal tree function_tree &lt;- force.ultrametric(function_tree, method=&quot;extend&quot;) %&gt;% ggtree(., size = 0.3) *************************************************************** * Note: * * force.ultrametric does not include a formal method to * * ultrametricize a tree &amp; should only be used to coerce * * a phylogeny that fails is.ultrametric due to rounding -- * * not as a substitute for formal rate-smoothing methods. * *************************************************************** #Add phylum colors next to the tree tips function_tree &lt;- gheatmap(function_tree, phylum_heatmap, offset=0, width=0.1, colnames=FALSE) + scale_fill_manual(values=phylum_colors) + labs(fill=&quot;Phylum&quot;) #Reset fill scale to use a different colour profile in the heatmap function_tree &lt;- function_tree + new_scale_fill() #Add functions heatmap function_tree &lt;- gheatmap(function_tree, function_table, offset=0.5, width=3.5, colnames=FALSE) + vexpand(.08) + coord_cartesian(clip = &quot;off&quot;) + scale_fill_gradient(low = &quot;#f4f4f4&quot;, high = &quot;steelblue&quot;, na.value=&quot;white&quot;) + labs(fill=&quot;GIFT&quot;) #Reset fill scale to use a different colour profile in the heatmap function_tree &lt;- function_tree + new_scale_fill() function_tree ## Functional ordination Rtsne(X=cluster_kegg_max %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;), dims = 2, check_duplicates = FALSE)$Y %&gt;% as.data.frame() %&gt;% mutate(secondary_cluster=cluster_kegg_max$secondary_cluster) %&gt;% rename(tSNE1=V1,tSNE2=V2) %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) %&gt;% left_join(cluster_prevalence,by=&quot;secondary_cluster&quot;) %&gt;% ggplot(aes(x = tSNE1, y = tSNE2, color = Phylum, size=n_strategy))+ geom_point(shape=16, alpha=0.7) + scale_color_manual(values=phylum_colors) + theme_minimal() + labs(color=&quot;Phylum&quot;, size=&quot;Number of strategies&quot;) + guides(color = guide_legend(override.aes = list(size = 5))) 5.2 Trait recovery all_strategy_clusters &lt;- cluster_prevalence %&gt;% filter(n_strategy==10) %&gt;% pull(secondary_cluster) # Filter clusters recovered in all strategies cluster_kegg_max_filt &lt;- cluster_kegg_max %&gt;% filter(secondary_cluster %in% all_strategy_clusters) cluster_kegg_proportion_max &lt;- cluster_kegg %&gt;% filter(!is.na(secondary_cluster)) %&gt;% filter(secondary_cluster %in% all_strategy_clusters) %&gt;% group_by(secondary_cluster) %&gt;% mutate(across(where(is.numeric), ~ . / max(., na.rm = TRUE))) %&gt;% ungroup() %&gt;% rowwise() %&gt;% mutate(average = rowMeans(across(where(is.numeric)), na.rm = TRUE)) %&gt;% select(secondary_cluster,overall_strategy,average) cluster_kegg_proportion_max %&gt;% group_by(overall_strategy) %&gt;% summarise(mean=mean(average),sd=sd(average)) %&gt;% tt() tinytable_m4rhvuw4cyoq6uyf3kv0 .table td.tinytable_css_ct7qapm2lutzhx35qykv, .table th.tinytable_css_ct7qapm2lutzhx35qykv { border-bottom: solid 0.1em #d3d8dc; } overall_strategy mean sd coassembly_animal 0.9669740 0.03788229 coassembly_timepoint_all 0.9642462 0.04627303 coassembly_timepoint_cage 0.9695513 0.03515391 multicoverage_animal 0.9686745 0.03929870 multicoverage_timepoint_all 0.9453590 0.08602739 multicoverage_timepoint_cage 0.9704001 0.03229802 multisplit_animal 0.9595563 0.03673926 multisplit_timepoint_all 0.9760940 0.02945151 multisplit_timepoint_cage 0.9589287 0.03776599 single_coverage 0.9622317 0.03741529 cluster_kegg_proportion_max %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) %&gt;% ggplot(aes(x = average, y = overall_strategy, group=overall_strategy, color=Phylum)) + geom_boxplot(color=&quot;#999999&quot;, fill=&quot;#f4f4f4&quot;, outlier.shape = NA) + scale_color_manual(values=phylum_colors[-c(1,4,6,7)]) + xlim(0.8, 1)+ geom_jitter(alpha=0.3) + theme_classic() + theme() + labs(x=&quot;Function recovery&quot;, y=&quot;Strategy&quot;) cluster_kegg_proportion_max %&gt;% mutate(secondary_cluster=factor(secondary_cluster,levels=rev(cluster_tree$tip.label[cluster_tree$tip.label %in% cluster_kegg_proportion_max$secondary_cluster]))) %&gt;% ggplot(aes(x=secondary_cluster,y=overall_strategy,fill=average))+ geom_tile()+ scale_fill_distiller(palette = &quot;YlGnBu&quot;, direction = -1) + theme_minimal() + theme(axis.text.x = element_blank(), axis.title.y = element_blank()) + labs(y=&quot;Strategy&quot;,x=&quot;Clusters&quot;,fill=&quot;Function recovery&quot;) completeness_stats &lt;- bin_metadata %&gt;% select(genome,completeness,contamination,overall_strategy, assembly, size) %&gt;% group_by(overall_strategy) %&gt;% summarise(completeness=mean(completeness)) functions_stats &lt;- cluster_kegg_proportion_max %&gt;% group_by(overall_strategy) %&gt;% summarise(functions=mean(average)) full_join(completeness_stats,functions_stats,by=&quot;overall_strategy&quot;) %&gt;% ggplot(aes(x=functions,y=completeness, color=overall_strategy))+ geom_point()+ scale_color_manual(values=strategy_colors)+ theme_minimal() "],["alpha-diversity.html", "6 Alpha diversity", " 6 Alpha diversity load(&quot;data/data.Rdata&quot;) alpha_div &lt;- tibble() for (strategy in strategies){ bin_counts_norm_filt_strategy &lt;- bin_counts_norm_filt %&gt;% filter(overall_strategy == {{strategy}}) %&gt;% select(secondary_cluster,sample,read_count) %&gt;% pivot_wider(names_from = &quot;sample&quot;, values_from = &quot;read_count&quot;, values_fn = sum) %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;) cluster_tree_strategy &lt;- keep.tip(cluster_tree, tip=rownames(bin_counts_norm_filt_strategy)) q0n &lt;- bin_counts_norm_filt_strategy %&gt;% hilldiv(.,q=0) %&gt;% as.numeric() q1n &lt;- bin_counts_norm_filt_strategy %&gt;% hilldiv(.,q=1) %&gt;% as.numeric() q1p &lt;- bin_counts_norm_filt_strategy %&gt;% hilldiv(.,q=1,tree=cluster_tree_strategy) %&gt;% as.numeric() alpha_div_strategy &lt;- tibble(strategy=strategy,sample=colnames(bin_counts_norm_filt_strategy),richness=q0n,neutral=round(q1n,3),phylo=round(q1p,3)) alpha_div&lt;- bind_rows(alpha_div,alpha_div_strategy) } save(alpha_div,file=&quot;results/alpha_diversity.Rdata&quot;) load(&quot;results/alpha_diversity.Rdata&quot;) alpha_div %&gt;% pivot_longer(!c(strategy,sample),names_to = &quot;metric&quot;,values_to = &quot;value&quot;) %&gt;% mutate(metric=factor(metric,levels=c(&quot;richness&quot;,&quot;neutral&quot;,&quot;phylo&quot;))) %&gt;% mutate(strategy=factor(strategy,levels=rev(strategies))) %&gt;% ggplot(aes(y=strategy, x=value, color=strategy))+ geom_boxplot(outlier.shape = NA)+ geom_jitter(alpha=0.1)+ scale_color_manual(values=strategy_colors)+ facet_nested(. ~ metric, scales = &quot;free&quot;) + theme_minimal() + labs(x=&quot;Diversity&quot;,y=&quot;Strategy&quot;) + theme(legend.position = &quot;none&quot;) load(&quot;results/alpha_diversity.Rdata&quot;) alpha_test &lt;- alpha_div %&gt;% pivot_longer(!c(strategy,sample),names_to = &quot;metric&quot;,values_to = &quot;value&quot;) %&gt;% mutate(strategy=factor(strategy,levels=strategies)) %&gt;% filter(metric==&quot;richness&quot;) %&gt;% lmerTest::lmer(value ~ strategy + (1 | sample), data = .) alpha_test %&gt;% broom.mixed::tidy() # A tibble: 12 × 8 effect group term estimate std.error statistic df p.value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fixed &lt;NA&gt; (Intercept) 65.8 0.872 75.5 325. 3.42e-208 2 fixed &lt;NA&gt; strategymulticoverage_animal 0.867 1.23 0.703 325. 4.82e- 1 3 fixed &lt;NA&gt; strategymulticoverage_timepoint_all 0.787 1.23 0.638 325. 5.24e- 1 4 fixed &lt;NA&gt; strategymulticoverage_timepoint_cage -7.82 1.23 -6.34 325. 7.48e- 10 5 fixed &lt;NA&gt; strategycoassembly_animal 12.5 0.290 43.2 1192. 3.27e-246 6 fixed &lt;NA&gt; strategycoassembly_timepoint_all 13.3 0.290 45.8 1192. 4.45e-265 7 fixed &lt;NA&gt; strategycoassembly_timepoint_cage 11.9 0.290 40.9 1192. 1.78e-229 8 fixed &lt;NA&gt; strategymultisplit_animal -8.67 1.23 -7.04 325. 1.17e- 11 9 fixed &lt;NA&gt; strategymultisplit_timepoint_all -1.93 1.23 -1.56 325. 1.19e- 1 10 fixed &lt;NA&gt; strategymultisplit_timepoint_cage -3.08 1.23 -2.50 325. 1.30e- 2 11 ran_pars sample sd__(Intercept) 10.4 NA NA NA NA 12 ran_pars Residual sd__Observation 2.51 NA NA NA NA alpha_test %&gt;% anova() %&gt;% broom::tidy() # A tibble: 1 × 7 term sumsq meansq NumDF DenDF statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 strategy 31409. 3490. 9 893. 553. 0 load(&quot;results/alpha_diversity.Rdata&quot;) alpha_test &lt;- alpha_div %&gt;% pivot_longer(!c(strategy,sample),names_to = &quot;metric&quot;,values_to = &quot;value&quot;) %&gt;% mutate(strategy=factor(strategy,levels=strategies)) %&gt;% filter(metric==&quot;neutral&quot;) %&gt;% lmerTest::lmer(value ~ strategy + (1 | sample), data = .) alpha_test %&gt;% broom.mixed::tidy() # A tibble: 12 × 8 effect group term estimate std.error statistic df p.value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fixed &lt;NA&gt; (Intercept) 29.6 0.630 47.0 303. 3.51e-141 2 fixed &lt;NA&gt; strategymulticoverage_animal -0.610 0.892 -0.685 303. 4.94e- 1 3 fixed &lt;NA&gt; strategymulticoverage_timepoint_all -0.519 0.892 -0.582 303. 5.61e- 1 4 fixed &lt;NA&gt; strategymulticoverage_timepoint_cage -3.18 0.892 -3.56 303. 4.23e- 4 5 fixed &lt;NA&gt; strategycoassembly_animal 1.61 0.0921 17.5 1192. 3.02e- 61 6 fixed &lt;NA&gt; strategycoassembly_timepoint_all 1.59 0.0921 17.2 1192. 1.67e- 59 7 fixed &lt;NA&gt; strategycoassembly_timepoint_cage 1.61 0.0921 17.4 1192. 7.53e- 61 8 fixed &lt;NA&gt; strategymultisplit_animal -3.95 0.892 -4.43 303. 1.32e- 5 9 fixed &lt;NA&gt; strategymultisplit_timepoint_all -0.768 0.892 -0.861 303. 3.90e- 1 10 fixed &lt;NA&gt; strategymultisplit_timepoint_cage -1.56 0.892 -1.74 303. 8.21e- 2 11 ran_pars sample sd__(Intercept) 7.68 NA NA NA NA 12 ran_pars Residual sd__Observation 0.798 NA NA NA NA alpha_test %&gt;% anova() %&gt;% broom::tidy() # A tibble: 1 × 7 term sumsq meansq NumDF DenDF statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 strategy 1900. 211. 9 893. 332. 1.17e-277 load(&quot;results/alpha_diversity.Rdata&quot;) alpha_test &lt;- alpha_div %&gt;% pivot_longer(!c(strategy,sample),names_to = &quot;metric&quot;,values_to = &quot;value&quot;) %&gt;% mutate(strategy=factor(strategy,levels=strategies)) %&gt;% filter(metric==&quot;phylo&quot;) %&gt;% lmerTest::lmer(value ~ strategy + (1 | sample), data = .) alpha_test %&gt;% broom.mixed::tidy() # A tibble: 12 × 8 effect group term estimate std.error statistic df p.value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 fixed &lt;NA&gt; (Intercept) 4.74 0.0572 82.8 313. 6.02e-215 2 fixed &lt;NA&gt; strategymulticoverage_animal -0.0705 0.0809 -0.871 313. 3.84e- 1 3 fixed &lt;NA&gt; strategymulticoverage_timepoint_all -0.0425 0.0809 -0.526 313. 6.00e- 1 4 fixed &lt;NA&gt; strategymulticoverage_timepoint_cage -0.226 0.0809 -2.80 313. 5.46e- 3 5 fixed &lt;NA&gt; strategycoassembly_animal 0.118 0.0142 8.28 1192. 3.38e- 16 6 fixed &lt;NA&gt; strategycoassembly_timepoint_all 0.189 0.0142 13.3 1192. 1.15e- 37 7 fixed &lt;NA&gt; strategycoassembly_timepoint_cage 0.0591 0.0142 4.15 1192. 3.53e- 5 8 fixed &lt;NA&gt; strategymultisplit_animal -0.256 0.0809 -3.16 313. 1.74e- 3 9 fixed &lt;NA&gt; strategymultisplit_timepoint_all -0.0337 0.0809 -0.417 313. 6.77e- 1 10 fixed &lt;NA&gt; strategymultisplit_timepoint_cage -0.0757 0.0809 -0.935 313. 3.50e- 1 11 ran_pars sample sd__(Intercept) 0.690 NA NA NA NA 12 ran_pars Residual sd__Observation 0.123 NA NA NA NA alpha_test %&gt;% anova() %&gt;% broom::tidy() # A tibble: 1 × 7 term sumsq meansq NumDF DenDF statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 strategy 10.2 1.13 9 893. 74.5 2.05e-102 "],["beta-diversity.html", "7 Beta diversity 7.1 Diversity calculations 7.2 Procrustes analysis", " 7 Beta diversity load(&quot;data/data.Rdata&quot;) 7.1 Diversity calculations beta_div &lt;- list() n=0 for (strategy in strategies){ n=n+1 bin_counts_norm_filt_strategy &lt;- bin_counts_norm_filt %&gt;% filter(overall_strategy == {{strategy}}) %&gt;% select(secondary_cluster,sample,read_count) %&gt;% pivot_wider(names_from = &quot;sample&quot;, values_from = &quot;read_count&quot;, values_fn = sum) %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;) cluster_tree_strategy &lt;- keep.tip(cluster_tree, tip=rownames(bin_counts_norm_filt_strategy)) beta_q0n &lt;- bin_counts_norm_filt_strategy %&gt;% hillpair(., q=0, metric=&quot;C&quot;) %&gt;% as.dist() beta_q1n &lt;- bin_counts_norm_filt_strategy %&gt;% hillpair(., q=1, metric=&quot;C&quot;) beta_q1p &lt;- bin_counts_norm_filt_strategy %&gt;% hillpair(., q=1, metric=&quot;C&quot;, tree = cluster_tree_strategy) beta_div_strategy &lt;- list(q0n=beta_q0n,q1n=beta_q1n,q1p=beta_q1p) beta_div[[n]] &lt;- beta_div_strategy } names(beta_div) &lt;- strategies save(beta_div,file=&quot;results/beta_diversity.Rdata&quot;) 7.2 Procrustes analysis 7.2.1 Richness load(&quot;results/beta_diversity.Rdata&quot;) #Generate NMDS ordinations nmds_q0n &lt;- list() for(strategy in strategies){ set.seed(1) nmds_q0n[[strategy]] &lt;- beta_div[[strategy]]$q0n %&gt;% metaMDS(.,trymax = 999, k=2, trace=0) } #Procrustes analysis combinations &lt;- expand.grid(seq(1, 10), seq(1, 10)) protest_q0n &lt;- tibble( v1 = character(), v2 = character(), scale = numeric(), sig = numeric() ) for(i in c(1:100)){ x&lt;-combinations[i,1] y&lt;-combinations[i,2] protest &lt;- protest(nmds_q0n[[x]], nmds_q0n[[y]]) protest_q0n &lt;- bind_rows(protest_q0n,tibble(v1=names(nmds_q0n)[x],v2=names(nmds_q0n)[y],scale=protest$scale,sig=protest$signif)) } colfunc&lt;-colorRampPalette(c(&quot;#44ce1b&quot;,&quot;#bbdb44&quot;,&quot;#f7e379&quot;,&quot;#f2a134&quot;,&quot;#e51f1f&quot;)) protest_q0n %&gt;% ggplot(aes(x=v1,y=v2,fill=sig)) + geom_tile(colour = &quot;white&quot;) + scale_fill_gradientn(colours=colfunc(10)) + theme_minimal() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x=NULL,y=NULL) 7.2.2 Neutral load(&quot;results/beta_diversity.Rdata&quot;) #Generate NMDS ordinations nmds_q1n &lt;- list() for(strategy in strategies){ set.seed(1) nmds_q1n[[strategy]] &lt;- beta_div[[strategy]]$q1n %&gt;% metaMDS(.,trymax = 999, k=2, trace=0) } #Procrustes analysis combinations &lt;- expand.grid(seq(1, 10), seq(1, 10)) protest_q1n &lt;- tibble( v1 = character(), v2 = character(), scale = numeric(), sig= numeric() ) for(i in c(1:100)){ x&lt;-combinations[i,1] y&lt;-combinations[i,2] protest &lt;- protest(nmds_q1n[[x]], nmds_q1n[[y]]) protest_q1n &lt;- bind_rows(protest_q1n,tibble(v1=names(nmds_q1n)[x],v2=names(nmds_q1n)[y],scale=protest$scale,sig=protest$signif)) } colfunc&lt;-colorRampPalette(c(&quot;#44ce1b&quot;,&quot;#bbdb44&quot;,&quot;#f7e379&quot;,&quot;#f2a134&quot;,&quot;#e51f1f&quot;)) protest_q1n %&gt;% ggplot(aes(x=v1,y=v2,fill=sig)) + geom_tile(colour = &quot;white&quot;) + scale_fill_gradientn(colours=colfunc(10)) + theme_minimal() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x=NULL,y=NULL) 7.2.3 Phylogenetic load(&quot;results/beta_diversity.Rdata&quot;) #Generate NMDS ordinations nmds_q1p &lt;- list() for(strategy in strategies){ set.seed(1) nmds_q1p[[strategy]] &lt;- beta_div[[strategy]]$q1p %&gt;% metaMDS(.,trymax = 999, k=2, trace=0) } #Procrustes analysis combinations &lt;- expand.grid(seq(1, 10), seq(1, 10)) protest_q1p &lt;- tibble( v1 = character(), v2 = character(), scale = numeric(), sig= numeric() ) for(i in c(1:100)){ x&lt;-combinations[i,1] y&lt;-combinations[i,2] protest &lt;- protest(nmds_q1p[[x]], nmds_q1p[[y]]) protest_q1p &lt;- bind_rows(protest_q1p,tibble(v1=names(nmds_q1p)[x],v2=names(nmds_q1p)[y],scale=protest$scale,sig=protest$signif)) } colfunc&lt;-colorRampPalette(c(&quot;#44ce1b&quot;,&quot;#bbdb44&quot;,&quot;#f7e379&quot;,&quot;#f2a134&quot;,&quot;#e51f1f&quot;)) protest_q1p %&gt;% ggplot(aes(x=v1,y=v2,fill=sig)) + geom_tile(colour = &quot;white&quot;) + scale_fill_gradientn(colours=colfunc(10)) + theme_minimal() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(x=NULL,y=NULL) "],["case-study-1.html", "8 Case study 1", " 8 Case study 1 sample_list &lt;- bin_counts %&gt;% select(sample) %&gt;% unique() %&gt;% mutate(animal=substr(sample,1,5)) %&gt;% mutate(timepoint=substr(sample,6,7)) %&gt;% filter(timepoint %in% c(&quot;OP&quot;,&quot;HR&quot;,&quot;CR&quot;)) %&gt;% group_by(animal) %&gt;% group_split() %&gt;% set_names(map_chr(., ~.x$animal[1])) animals &lt;- names(sample_list) beta_dissimilarities &lt;- tibble() for (strategy in strategies){ for (animal in animals){ samples &lt;- sample_list[[animal]] %&gt;% pull(sample) counts &lt;- bin_counts_norm_filt %&gt;% filter(overall_strategy == {{strategy}}) %&gt;% filter(sample %in% samples) %&gt;% select(secondary_cluster,sample,read_count) %&gt;% pivot_wider(names_from = &quot;sample&quot;, values_from = &quot;read_count&quot;, values_fn = sum) %&gt;% mutate(across(where(is.double), ~ ./sum(.))) %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;) tree &lt;- cluster_tree %&gt;% keep.tip(., tip=rownames(counts)) diss &lt;- tibble(animal=animal, strategy=strategy) %&gt;% mutate(richness=hilldiss(data=counts,metric=&quot;C&quot;,q=0)) %&gt;% mutate(neutral=hilldiss(data=counts,metric=&quot;C&quot;,q=1)) %&gt;% mutate(phylo=hilldiss(data=counts,metric=&quot;C&quot;,q=1,tree=tree)) beta_dissimilarities &lt;- bind_rows(beta_dissimilarities,diss) } } #Extract metadata from sample names beta_dissimilarities &lt;- beta_dissimilarities %&gt;% mutate(sex=substr(animal,4,4)) %&gt;% mutate(cage=substr(animal,1,3)) beta_tests &lt;- beta_dissimilarities %&gt;% group_by(strategy) %&gt;% summarise( model_richness = list(kruskal.test(richness ~ sex)), model_neutral = list(kruskal.test(neutral ~ sex)), model_phylo = list(kruskal.test(phylo ~ sex))) %&gt;% ungroup() %&gt;% select(strategy,model_richness,model_neutral,model_phylo) %&gt;% unique() %&gt;% rowwise() %&gt;% mutate(richess_estimate = unlist(model_richness)[1],richness_pvalue = unlist(model_richness)[3]) %&gt;% mutate(neutral_estimate = unlist(model_neutral)[1],neutral_pvalue = unlist(model_neutral)[3]) %&gt;% mutate(phylo_estimate = unlist(model_phylo)[1],phylo_estimate = unlist(model_phylo)[3]) beta_tests %&gt;% select(strategy,richess_estimate,richness_pvalue) %&gt;% tt() |&gt; style_tt( i = which(beta_tests$richness_pvalue &gt; 0.05), background = &quot;#ffd7cf&quot;) tinytable_vkk5lphy6ucvauuzlumt .table td.tinytable_css_mwzkt94i8u5q01g8heby, .table th.tinytable_css_mwzkt94i8u5q01g8heby { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_6jlbs798aq91ofsf4v1j, .table th.tinytable_css_6jlbs798aq91ofsf4v1j { background-color: #ffd7cf; } strategy richess_estimate richness_pvalue coassembly_animal 2.41989319092123 0.119803098705944 coassembly_timepoint_all 3.8825841863225 0.0487894065141656 coassembly_timepoint_cage 3.96563543289562 0.0464379918989709 multicoverage_animal 1.93075268817205 0.164676120550994 multicoverage_timepoint_all 1.44720367897939 0.228977176372437 multicoverage_timepoint_cage 3.1824912827361 0.0744311670870192 multisplit_animal 4.38752688172042 0.0362027910223606 multisplit_timepoint_all 4.38850318943776 0.0361820649321886 multisplit_timepoint_cage 4.92430107526883 0.0264815118804495 single_coverage 4.04778222815606 0.0442294151169678 beta_dissimilarities %&gt;% mutate(strategy=factor(strategy,levels=strategies)) %&gt;% ggplot(aes(x=sex,y=richness,color=strategy, fill=strategy)) + geom_boxplot(outlier.shape = NA)+ scale_color_manual(values=strategy_colors)+ scale_fill_manual(values=paste0(strategy_colors,&quot;50&quot;))+ facet_grid(~strategy)+ theme_light() + theme(legend.position = &quot;none&quot;) "],["case-study-2.html", "9 Case study 2", " 9 Case study 2 community_kegg_list &lt;- list() for (strategy in strategies){ cluster_kegg_strategy &lt;- cluster_kegg %&gt;% filter(overall_strategy == strategy) %&gt;% select(-overall_strategy) %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;) bin_counts_norm_filt_strategy &lt;- bin_counts_norm_filt %&gt;% mutate(timepoint=substr(sample,6,7)) %&gt;% filter(timepoint %in% c(&quot;HT&quot;,&quot;CT&quot;)) %&gt;% filter(secondary_cluster %in% rownames(cluster_kegg_strategy)) %&gt;% select(secondary_cluster,sample,read_count) %&gt;% pivot_wider(names_from = &quot;sample&quot;, values_from = &quot;read_count&quot;, values_fn = sum) %&gt;% mutate(across(where(is.double), ~ ./sum(.))) community_kegg_table &lt;- c() samples &lt;- colnames(bin_counts_norm_filt_strategy)[-1] for(sample in samples){ community_kegg &lt;- colSums(sweep(cluster_kegg_strategy[bin_counts_norm_filt_strategy$secondary_cluster,],1,bin_counts_norm_filt_strategy %&gt;% pull({{sample}}), FUN=&quot;*&quot;)) community_kegg_table &lt;- rbind(community_kegg_table,community_kegg) } rownames(community_kegg_table) &lt;- samples community_kegg_list[[strategy]] &lt;- community_kegg_table } community_kegg_comparison_list &lt;- list() for (strategy in strategies){ community_kegg_comparison&lt;- community_kegg_list[[strategy]] %&gt;% as.data.frame() %&gt;% rownames_to_column(var=&quot;sample&quot;) %&gt;% pivot_longer(!sample,names_to=&quot;trait&quot;,values_to=&quot;value&quot;) %&gt;% mutate(animal=substr(sample,1,5)) %&gt;% mutate(treatment=substr(sample,6,7)) %&gt;% group_by(trait) %&gt;% mutate(meanvalue=mean(value)) %&gt;% filter(meanvalue&gt;0.3) %&gt;% mutate(model_result = list(lmerTest::lmer(value ~ treatment + (1 | animal)))) %&gt;% ungroup() %&gt;% select(trait,model_result) %&gt;% unique() %&gt;% mutate(estimate = map_dbl(model_result, ~broom.mixed::tidy(.) %&gt;% filter(term == &quot;treatmentHT&quot;) %&gt;% pull(estimate))) %&gt;% mutate(p_value = map_dbl(model_result, ~broom.mixed::tidy(.) %&gt;% filter(term == &quot;treatmentHT&quot;) %&gt;% pull(p.value))) %&gt;% mutate(p_value_adj = p.adjust(p_value, method = &quot;bonferroni&quot;)) %&gt;% select(trait, estimate, p_value_adj) community_kegg_comparison_list[[strategy]] &lt;- community_kegg_comparison } all_p_values &lt;- imap_dfr(community_kegg_comparison_list, ~ .x %&gt;% mutate(strategy = .y)) all_p_values %&gt;% mutate(trend=ifelse(p_value_adj&lt;0.05 &amp; estimate&gt;0,&quot;hot&quot;,&quot;neutral&quot;)) %&gt;% mutate(trend=ifelse(p_value_adj&lt;0.05 &amp; estimate&lt;0,&quot;cold&quot;,trend)) %&gt;% group_by(trait, trend) %&gt;% summarise(count = n()) %&gt;% ungroup() %&gt;% pivot_wider(values_from=&quot;count&quot;,names_from=&quot;trend&quot;) %&gt;% tt() tinytable_nc9ppmxq8nk43hr6n2b7 .table td.tinytable_css_cu6ajmu6riipwv8kx9fc, .table th.tinytable_css_cu6ajmu6riipwv8kx9fc { border-bottom: solid 0.1em #d3d8dc; } trait neutral hot cold M00001 10 NA NA M00002 10 NA NA M00003 6 4 NA M00004 NA NA 10 M00005 6 4 NA M00007 NA NA 10 M00009 NA 10 NA M00011 NA 10 NA M00015 10 NA NA M00016 3 NA 7 M00017 8 NA 2 M00018 9 NA 1 M00019 10 NA NA M00020 10 NA NA M00021 3 NA 7 M00022 NA NA 10 M00023 10 NA NA M00024 10 NA NA M00025 10 NA NA M00026 10 NA NA M00028 NA NA 10 M00029 NA NA 10 M00035 10 NA NA M00045 10 NA NA M00048 10 NA NA M00049 10 NA NA M00050 3 7 NA M00051 10 NA NA M00052 NA 10 NA M00053 NA 10 NA M00060 NA 10 NA M00061 8 NA 2 M00063 NA 10 NA M00082 4 NA 6 M00083 10 NA NA M00086 2 8 NA M00089 NA NA 10 M00093 9 1 NA M00096 1 NA 9 M00115 NA 10 NA M00116 NA 10 NA M00119 NA 10 NA M00120 10 NA NA M00121 10 NA NA M00122 10 NA NA M00123 NA 10 NA M00124 NA 10 NA M00125 NA 10 NA M00126 NA 10 NA M00127 NA 10 NA M00140 10 NA NA M00141 NA 10 NA M00165 10 NA NA M00166 8 2 NA M00167 9 NA 1 M00169 1 9 NA M00172 1 9 NA M00173 NA 10 NA M00178 10 NA NA M00179 10 NA NA M00183 10 NA NA M00260 10 NA NA M00307 10 NA NA M00308 4 NA 6 M00345 10 NA NA M00346 6 4 NA M00368 7 3 NA M00373 NA 10 NA M00377 NA NA 10 M00432 10 NA NA M00525 5 NA 5 M00526 10 NA NA M00527 10 NA NA M00532 1 9 NA M00535 10 NA NA M00549 10 NA NA M00552 10 NA NA M00565 NA NA 10 M00570 10 NA NA M00572 10 NA NA M00573 NA 10 NA M00577 NA 10 NA M00579 1 NA 9 M00609 7 NA 3 M00627 10 NA NA M00631 10 NA NA M00632 8 2 NA M00651 10 NA NA M00741 NA 10 NA M00793 10 NA NA M00841 NA 10 NA M00842 NA 10 NA M00843 NA 10 NA M00844 NA NA 10 M00845 NA NA 10 M00846 10 NA NA M00854 NA NA 10 M00855 5 NA 5 M00866 NA 10 NA M00874 10 NA NA #Maximum number of significances all_p_values %&gt;% mutate(trend=ifelse(p_value_adj&lt;0.05 &amp; estimate&gt;0,&quot;hot&quot;,&quot;neutral&quot;)) %&gt;% mutate(trend=ifelse(p_value_adj&lt;0.05 &amp; estimate&lt;0,&quot;cold&quot;,trend)) %&gt;% group_by(strategy, trend) %&gt;% summarise(count = n()) %&gt;% ungroup() %&gt;% pivot_wider(values_from=&quot;count&quot;,names_from=&quot;trend&quot;) %&gt;% tt() tinytable_pw51cnwc39esgn34al3r .table td.tinytable_css_dbhx2nfagq9qeeuzcn3u, .table th.tinytable_css_dbhx2nfagq9qeeuzcn3u { border-bottom: solid 0.1em #d3d8dc; } strategy cold hot neutral coassembly_animal 17 31 52 coassembly_timepoint_all 18 27 55 coassembly_timepoint_cage 15 33 52 multicoverage_animal 16 32 52 multicoverage_timepoint_all 17 27 56 multicoverage_timepoint_cage 19 27 54 multisplit_animal 21 32 47 multisplit_timepoint_all 17 33 50 multisplit_timepoint_cage 16 29 55 single_coverage 17 31 52 #Non-ambiguous trends %&gt;% mutate(ambituous = !(neutral == 10 | hot == 10 | cold == 10)) %&gt;% filter(ambituous == FALSE) %&gt;% nrow() [1] 75 #Ambiguous trends %&gt;% mutate(ambituous = !(neutral == 10 | hot == 10 | cold == 10)) %&gt;% filter(is.na(ambituous)) %&gt;% nrow() [1] 25 functional_trends &lt;- all_p_values %&gt;% mutate(trend=ifelse(p_value_adj&lt;0.05 &amp; estimate&gt;0,&quot;hot&quot;,&quot;neutral&quot;)) %&gt;% mutate(trend=ifelse(p_value_adj&lt;0.05 &amp; estimate&lt;0,&quot;cold&quot;,trend)) %&gt;% select(trait,strategy,trend) %&gt;% pivot_wider(names_from=&quot;trait&quot;,values_from = &quot;trend&quot;) %&gt;% column_to_rownames(var=&quot;strategy&quot;) functional_tree &lt;- all_p_values %&gt;% select(-p_value_adj) %&gt;% pivot_wider(names_from=&quot;trait&quot;,values_from=&quot;estimate&quot;) %&gt;% column_to_rownames(var=&quot;strategy&quot;) %&gt;% dist() %&gt;% hclust() %&gt;% ggtree() gheatmap(functional_tree, functional_trends, offset=0.15, colnames=FALSE, width=8) + geom_tiplab(size=2, align=TRUE) + scale_fill_manual(values=c(&quot;#78bacf&quot;,&quot;#e0766e&quot;,&quot;#f4f4f4&quot;)) "],["computational-runtimes-analysis.html", "10 Computational runtimes analysis 10.1 Load packages and import/wrangle data 10.2 Create figure A) 10.3 Create figure B) 10.4 Patch together into the final figure", " 10 Computational runtimes analysis 10.1 Load packages and import/wrangle data # Individual/coassembly had data collected using snakemake benchmark, so load these ind &lt;- read_delim(&quot;data/runtime/ind_benchmark.tsv&quot;, col_names = c(&quot;job&quot;, &quot;time_s&quot;, &quot;strategy&quot;)) coassembly &lt;- read_delim(&quot;data/runtime/coassembly_benchmark.tsv&quot;, col_names = c(&quot;job&quot;, &quot;time_s&quot;, &quot;strategy&quot;)) # import data into tidy format files &lt;- list.files(&quot;data/runtime&quot;, &quot;*report.tsv&quot;, full.names = T) import &lt;- function(file){ read_delim(file, col_names = c(&quot;job&quot;, &quot;time_s&quot;)) %&gt;% mutate(strategy = str_replace(file, &quot;data/runtime/&quot;, &quot;&quot;), strategy = str_replace_all(strategy, &quot;_report.tsv&quot;, &quot;&quot;)) } df &lt;- purrr::map(files, import) %&gt;% bind_rows() %&gt;% # Noticed an odd bug with snakemake report, 2 jobs have time values ~-900000000.. so filter them # Also noticed an outlier job with 824084 seconds (mean is 600 for that type) filter(time_s &gt; 0 &amp; time_s &lt; 800000) %&gt;% bind_rows(., ind, coassembly) %&gt;% mutate(job = str_replace_all(job, &quot;metaWRAP_binning&quot;, &quot;binning&quot;), job = str_replace_all(job, &quot;metaWRAP_refinement&quot;, &quot;refinement&quot;)) #Subset for comparable jobs jobs &lt;- c(&quot;binning&quot;, &quot;refinement&quot;, &quot;mapping&quot;, &quot;Assembly&quot;, &quot;assembly_mapping&quot;, &quot;checkm&quot;, &quot;vamb_multisplit&quot;) #filter by jobs of interest df_filt &lt;- df %&gt;% filter(job %in% jobs) %&gt;% mutate(job = str_replace_all(job, &quot;^mapping&quot;, &quot;assembly_mapping&quot;)) %&gt;% filter(strategy != &quot;cobinning_treat&quot;) #Add ind assembly times for cobinning/multi-split time_total_s_ind &lt;- df_filt %&gt;% filter(strategy == &quot;individual&quot; &amp; job == &quot;Assembly&quot;) %&gt;% pull(time_s) df_cobinning_cage_treat &lt;- tibble( strategy = &quot;cobinning_cage_treat&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_cobinning_long &lt;- tibble( strategy = &quot;cobinning_long&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_vamb_cage_treat &lt;- tibble( strategy = &quot;vamb_cage_treat&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_vamb_treat &lt;- tibble( strategy = &quot;vamb_treat&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_vamb_long &lt;- tibble( strategy = &quot;vamb_long&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_filt &lt;- df_filt %&gt;% bind_rows(df_cobinning_cage_treat, df_cobinning_long, df_vamb_cage_treat, df_vamb_treat, df_vamb_long) 10.2 Create figure A) #Get sum per strategy, and rename strategies for consisitency with manuscript df_sum &lt;- df_filt %&gt;% summarise(time_total_s = sum(time_s), .by = c(&quot;strategy&quot;, &quot;job&quot;)) #Plotting colours &lt;- c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#000000&quot;) fig_a &lt;- df_sum %&gt;% ggplot(aes(x = strategy, y = time_total_s / 3600, fill = job)) + geom_bar(stat = &quot;identity&quot;) + theme_classic() + theme(axis.text.x = element_text(angle = 45, size = 13, hjust = 1), axis.text.y = element_text(size = 14), axis.title = element_text(size = 14, face = &quot;bold&quot;), axis.title.x = element_blank()) + scale_fill_manual(values = colours) + labs(y = &quot;Time (hours)&quot;) #Save total time summary for another figure df_sum_total &lt;- df_sum %&gt;% summarise(time_total_s = sum(time_total_s), .by = strategy) write_delim(df_sum_total, &quot;data/runtime_total.tsv&quot;) 10.3 Create figure B) #sample size and treatment variables n1 = 15 n2 = 30 n3 = 60 n4 = 120 t1 = 1 t2 = 2 t3 = 3 t4 = 4 ################################################################################ ### individual assembly df_sum_total_ind &lt;- df_sum_total %&gt;% filter(strategy == &quot;single-coverage&quot;) %&gt;% mutate(sample_size = n2, n_treatments = t1, time_total_s = time_total_s / 5) #this creates sample_size 30, with time_total_s = 150 / 5 = 30 indiv &lt;- tibble( strategy = &quot;single-coverage&quot;, sample_size = c(n1, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4), n_treatments = c(t1, t1, t1, t2, t2, t2, t2, t3, t3, t3, t3, t4, t4, t4, t4) ) %&gt;% bind_rows(., df_sum_total_ind) %&gt;% mutate(time_total_s = (df_sum_total_ind$time_total_s / 30) * sample_size * n_treatments) #note df_sum_total_ind$time_total_s / 30 = estimated time per sample. fig_b &lt;- indiv %&gt;% ggplot(aes(x = sample_size, y = time_total_s / 3600, group = n_treatments, colour = n_treatments)) + geom_line() + geom_point() + scale_x_continuous(breaks = c(n1, n2, n3, n4)) + theme_classic() + theme(axis.title = element_text(face = &quot;bold&quot;, size = 14), axis.text = element_text(size = 14), axis.title.x = element_blank()) + ylab(&quot;Time (hours)&quot;) + ggtitle(&quot;Single-coverage binning&quot;) ################################################################################ ### coassembly df_sum_total_coa &lt;- df_sum_total %&gt;% filter(str_detect(strategy, &quot;^coassembly&quot;)) #Pull coassembly job times coassembly_df &lt;- df_filt %&gt;% filter(str_detect(strategy, &quot;^coassembly&quot;)) #Get mean times across coassembly strategies #obviously more samples = longer assemblies, but this is just a simple estimate #note: divide by n samples in coassembly mean_coassembly_t &lt;- coassembly_df %&gt;% filter(job == &quot;Assembly&quot; &amp; strategy == &quot;coassembly_timepoint_all&quot;) %&gt;% pull(time_s) %&gt;% mean() / 30 mean_coassembly_ct &lt;- coassembly_df %&gt;% filter(job == &quot;Assembly&quot; &amp; strategy == &quot;coassembly_timepoint_cage&quot;) %&gt;% pull(time_s) %&gt;% mean() / 5 mean_coassembly = (mean_coassembly_t + mean_coassembly_ct) / 2 #Do same for other jobs mean_mapping = coassembly_df %&gt;% filter(job == &quot;assembly_mapping&quot;) %&gt;% pull(time_s) %&gt;% mean() mean_binning_t &lt;- coassembly_df %&gt;% filter(job == &quot;binning&quot; &amp; strategy == &quot;coassembly_timepoint_all&quot;) %&gt;% pull(time_s) %&gt;% mean() / 30 mean_binning_ct &lt;- coassembly_df %&gt;% filter(job == &quot;binning&quot; &amp; strategy == &quot;coassembly_timepoint_cage&quot;) %&gt;% pull(time_s) %&gt;% mean() / 5 mean_binning = (mean_binning_t + mean_binning_ct) / 2 mean_refinement_t &lt;- coassembly_df %&gt;% filter(job == &quot;refinement&quot; &amp; strategy == &quot;coassembly_timepoint_all&quot;) %&gt;% pull(time_s) %&gt;% mean() / 30 mean_refinement_ct &lt;- coassembly_df %&gt;% filter(job == &quot;refinement&quot; &amp; strategy == &quot;coassembly_timepoint_cage&quot;) %&gt;% pull(time_s) %&gt;% mean() / 5 mean_refinement = (mean_refinement_t + mean_refinement_ct) / 2 #create tibble with estimatess coa &lt;- tibble( strategy = &quot;coassembly&quot;, sample_size = c(n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4), n_treatments = c(t1, t1, t1, t1, t2, t2, t2, t2, t3, t3, t3, t3, t4, t4, t4, t4) ) %&gt;% mutate(time_total_s = (mean_coassembly * sample_size * n_treatments) + (mean_mapping * sample_size * n_treatments) + (mean_binning * sample_size * n_treatments) + (mean_refinement * sample_size * n_treatments)) fig_c &lt;- coa %&gt;% ggplot(aes(x = sample_size, y = time_total_s / 3600, group = n_treatments, colour = n_treatments)) + geom_line() + geom_point() + scale_x_continuous(breaks = c(n1, n2, n3, n4)) + theme_classic() + theme(axis.title = element_text(face = &quot;bold&quot;, size = 14), axis.text = element_text(size = 14), axis.title.y = element_blank(), axis.title.x = element_blank()) + ggtitle(&quot;Coassembly&quot;) ################################################################################ ### multi-coverage binning df_cob_cage_treat &lt;- df_filt %&gt;% filter(strategy == &quot;multi-coverage_timepoint_cage&quot;) #mean time for assembly? mean_assembly &lt;- df_cob_cage_treat %&gt;% filter(job == &quot;Assembly&quot;) %&gt;% pull(time_s) %&gt;% mean() #mean time for mapping? mean_mapping &lt;- df_cob_cage_treat %&gt;% filter(job == &quot;assembly_mapping&quot;) %&gt;% pull(time_s) %&gt;% mean() #mean time for binning? mean_binning &lt;- df_cob_cage_treat %&gt;% filter(job == &quot;binning&quot;) %&gt;% pull(time_s) %&gt;% mean() #mean time for refinement? mean_refinement &lt;- df_cob_cage_treat %&gt;% filter(job == &quot;refinement&quot;) %&gt;% pull(time_s) %&gt;% mean() cob &lt;- tibble( strategy = &quot;multi-coverage binning&quot;, sample_size = c(n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4), n_treatments = c(t1, t1, t1, t1, t2, t2, t2, t2, t3, t3, t3, t3, t4, t4, t4, t4) ) %&gt;% mutate(time_total_s = (mean_assembly * sample_size * n_treatments) + (mean_mapping * (sample_size^2) * n_treatments) + (mean_binning * sample_size * n_treatments) + (mean_refinement * sample_size * n_treatments)) #scaling is sample_size, so sample_size^2 -- e.g. 2 samples = 2 * 2 mappings, 3 samples = 3 * 3 mappings, etc. fig_d &lt;- cob %&gt;% ggplot(aes(x = sample_size, y = time_total_s / 3600, group = n_treatments, colour = n_treatments)) + geom_line() + geom_point() + scale_x_continuous(breaks = c(n1, n2, n3, n4)) + theme_classic() + theme(axis.text = element_text(size = 14), axis.title = element_text(size = 14, face = &quot;bold&quot;)) + xlab(&quot;Number of samples&quot;) + ylab(&quot;Time (hours)&quot;) + ggtitle(&quot;Multi-coverage binning&quot;) ################################################################################ ### multi-split df_sum_total_ms &lt;- df_sum_total %&gt;% filter(str_detect(strategy, &quot;^multi-split&quot;)) df_ms_cage_treat &lt;- df_filt %&gt;% filter(strategy == &quot;multi-split_timepoint_cage&quot;) #mean time for assembly? mean_assembly &lt;- df_ms_cage_treat %&gt;% filter(job == &quot;Assembly&quot;) %&gt;% pull(time_s) %&gt;% mean() #How does checkm scale? checkm_t &lt;- df_filt %&gt;% filter(job == &quot;checkm&quot; &amp; strategy == &quot;multi-split_timepoint_all&quot;) checkm_ct &lt;- df_filt %&gt;% filter(job == &quot;checkm&quot; &amp; strategy == &quot;multi-split_timepoint_cage&quot;) treat &lt;- mean(checkm_t$time_s) cage_treat &lt;- mean(checkm_ct$time_s) #per sample? treat_ps &lt;- treat / 30 cage_treat_ps &lt;- cage_treat / 5 #pretty similar, so for simplicity (and given it&#39;s a fraction of the pipeline&#39;s time, let&#39;s assume the mean of the two) mean_checkm &lt;- (treat_ps + cage_treat_ps) / 2 #How does vamb_multisplit scale? vamb_multisplit_t &lt;- df_filt %&gt;% filter(job == &quot;vamb_multisplit&quot; &amp; strategy == &quot;multi-split_timepoint_all&quot;) vamb_multisplit_ct &lt;- df_filt %&gt;% filter(job == &quot;vamb_multisplit&quot; &amp; strategy == &quot;multi-split_timepoint_cage&quot;) treat &lt;- mean(vamb_multisplit_t$time_s) cage_treat &lt;- mean(vamb_multisplit_ct$time_s) #per sample? treat_ps &lt;- treat / 30 cage_treat_ps &lt;- cage_treat / 5 #pretty similar, so for simplicity (and given it&#39;s a fraction of the pipeline&#39;s time, let&#39;s assume the mean of the two) mean_vamb_multisplit &lt;- (treat_ps + cage_treat_ps) / 2 #How does assembly_mapping scale? mapping_t &lt;- df_filt %&gt;% filter(job == &quot;assembly_mapping&quot; &amp; strategy == &quot;multi-split_timepoint_all&quot;) mapping_ct &lt;- df_filt %&gt;% filter(job == &quot;assembly_mapping&quot; &amp; strategy == &quot;multi-split_timepoint_cage&quot;) treat &lt;- mean(mapping_t$time_s) cage_treat &lt;- mean(mapping_ct$time_s) # more samples combined = larger reference = longer mapping time # Find scaling exponent (note, treat has 30 samples, cage_treat 5) alpha &lt;- log(treat / cage_treat) / log(30 / 5) # Calculate a proportionality constant (choose cage_treat arbitrarily) k &lt;- cage_treat / (5) ^ alpha # Function to estimate time for 1 sample to map to combined references estimate_time &lt;- function(n) { k * (n) ^ alpha } # Estimation for 15, 30, 60, and 120 samples combined n_values &lt;- c(15, 30, 60, 120) mapping_estimated_times &lt;- sapply(n_values, estimate_time) # Create tibble with estimates ms &lt;- tibble( strategy = &quot;multi-split&quot;, sample_size = c(n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4), n_treatments = c(t1, t1, t1, t1, t2, t2, t2, t2, t3, t3, t3, t3, t4, t4, t4, t4) ) %&gt;% mutate(time_total_s = (mean_assembly * sample_size * n_treatments) + (mapping_estimated_times * sample_size * n_treatments) + (mean_vamb_multisplit * sample_size * n_treatments) + (mean_checkm * sample_size * n_treatments)) fig_e &lt;- ms %&gt;% ggplot(aes(x = sample_size, y = time_total_s / 3600, group = n_treatments, colour = n_treatments)) + geom_line() + geom_point() + scale_x_continuous(breaks = c(n1, n2, n3, n4)) + theme_classic() + theme(axis.title = element_text(face = &quot;bold&quot;, size = 14), axis.text = element_text(size = 14), axis.title.y = element_blank(),) + xlab(&quot;Number of samples&quot;) + ggtitle(&quot;Multi-split&quot;) 10.4 Patch together into the final figure fig_a / (fig_b + fig_c + fig_d + fig_e) + plot_layout(guides = &#39;collect&#39;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
