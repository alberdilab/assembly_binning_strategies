[["index.html", "AlberdiLab | Eisenhofer et al. in prep Contrasting assembly and binning strategies recover different genome catalogues 1 Introduction 1.1 Prepare the R environment", " AlberdiLab | Eisenhofer et al. in prep Contrasting assembly and binning strategies recover different genome catalogues Raphael Eisenhofer1, Antton Alberdi2, Ostaizka Aizpurua3 Latest update: 2024-05-08 1 Introduction This webbook contains all the code used for data analysis in study of the population-level metagenomic data of Podarcis muralis lizards across elevational gradients in various mountain ranges of the Pyrenees. 1.1 Prepare the R environment 1.1.1 Environment To reproduce all the analyses locally, clone this repository in your computer using: RStudio &gt; New Project &gt; Version Control &gt; Git And indicating the following git repository: https://github.com/alberdilab/assembly_binning_strategies.git Once the R project has been created, follow the instructions and code chunks shown in this webbook. 1.1.2 Libraries The following R packages are required for the data analysis. # Base library(R.utils) library(knitr) library(tidyverse) library(devtools) library(tinytable) library(rairtable) # For tree handling library(ape) library(phyloseq) library(phytools) # For plotting library(ggplot2) library(ggrepel) library(ggpubr) library(ggnewscale) library(gridExtra) library(ggtreeExtra) library(ggtree) library(ggh4x) library(patchwork) # For statistics library(spaa) library(vegan) library(Rtsne) library(geiger) library(hilldiv2) library(distillR) library(broom.mixed) #library(lmerTest) library(Hmsc) library(corrplot) University of Copenhagen, raphael.eisenhofer@sund.ku.dk↩︎ University of Copenhagen, antton.alberdi@sund.ku.dk↩︎ University of Copenhagen, ostaizka.aizpurua@sund.ku.dk↩︎ "],["data-preparation.html", "2 Data preparation 2.1 Metadata 2.2 Read counts 2.3 Cluster taxonomy 2.4 Cluster tree 2.5 Cluster counts 2.6 Cluster functions 2.7 Color scheme 2.8 Wrap working objects", " 2 Data preparation 2.1 Metadata bin_metadata &lt;- read_delim(&quot;data/bin_metadata.tsv.gz&quot;) %&gt;% rename(genome=bin_id) %&gt;% mutate(comp_cont = completeness - contamination) %&gt;% mutate(overall_strategy = case_when( overall_strategy == &quot;individual_individual_binning&quot; ~ &quot;single_coverage&quot;, overall_strategy == &quot;cobinning_longitudinal&quot; ~ &quot;multicoverage_animal&quot;, overall_strategy == &quot;cobinning_cage_treatment&quot; ~ &quot;multicoverage_timepoint_all&quot;, overall_strategy == &quot;cobinning_treatment&quot; ~ &quot;multicoverage_timepoint_cage&quot;, overall_strategy == &quot;coassembly_longitudinal&quot; ~ &quot;coassembly_animal&quot;, overall_strategy == &quot;coassembly_treatment&quot; ~ &quot;coassembly_timepoint_all&quot;, overall_strategy == &quot;coassembly_cage_treatment&quot; ~ &quot;coassembly_timepoint_cage&quot;, overall_strategy == &quot;multi_split_longitudinal&quot; ~ &quot;multisplit_animal&quot;, overall_strategy == &quot;multi_split_treatment&quot; ~ &quot;multisplit_timepoint_all&quot;, overall_strategy == &quot;multi_split_cage_treatment&quot; ~ &quot;multisplit_timepoint_cage&quot;, TRUE ~ overall_strategy)) %&gt;% mutate(assembly = case_when( assembly == &quot;individual&quot; ~ &quot;single_coverage&quot;, assembly == &quot;cobinning&quot; ~ &quot;multi_coverage&quot;, assembly == &quot;coassembly&quot; ~ &quot;coassembly&quot;, assembly == &quot;multi_split&quot; ~ &quot;multi_split&quot;, TRUE ~ assembly)) %&gt;% mutate(strategy = case_when( strategy == &quot;individual_binning&quot; ~ &quot;NA&quot;, strategy == &quot;longitudinal&quot; ~ &quot;animal&quot;, strategy == &quot;treatment&quot; ~ &quot;timepoint_all&quot;, strategy == &quot;cage_treatment&quot; ~ &quot;timepoint_cage&quot;, TRUE ~ strategy)) %&gt;% mutate(genome = str_replace(genome, &quot;individual&quot;, &quot;single_coverage&quot;), genome = str_replace(genome, &quot;cobinning_long&quot;, &quot;multicoverage_animal&quot;), genome = str_replace(genome, &quot;cobinning_treat&quot;, &quot;multicoverage_timepoint_all&quot;), genome = str_replace(genome, &quot;cobinning_cage_treat&quot;, &quot;multicoverage_timepoint_cage&quot;), genome = str_replace(genome, &quot;coassembly_long&quot;, &quot;coassembly_animal&quot;), genome = str_replace(genome, &quot;coassembly_treat&quot;, &quot;coassembly_timepoint_all&quot;), genome = str_replace(genome, &quot;coassembly_cage_treat&quot;, &quot;coassembly_timepoint_cage&quot;), genome = str_replace(genome, &quot;vamb_long&quot;, &quot;multisplit_animal&quot;), genome = str_replace(genome, &quot;vamb_treat&quot;, &quot;multisplit_timepoint_all&quot;), genome = str_replace(genome, &quot;vamb_cage_treat&quot;, &quot;multisplit_timepoint_cage&quot;)) strategies &lt;- c(&quot;single_coverage&quot;, &quot;multicoverage_animal&quot;, &quot;multicoverage_timepoint_all&quot;, &quot;multicoverage_timepoint_cage&quot;, &quot;coassembly_animal&quot;, &quot;coassembly_timepoint_all&quot;, &quot;coassembly_timepoint_cage&quot;, &quot;multisplit_animal&quot;, &quot;multisplit_timepoint_all&quot;, &quot;multisplit_timepoint_cage&quot;) all_counts &lt;- read_delim(&quot;data/all_counts.tsv&quot;, col_names = c(&quot;sample&quot;, &quot;total_count&quot;)) %&gt;% mutate(total_count = total_count * 2) 2.2 Read counts import_count_data &lt;- function(strategy) { read_delim(file = paste0(&#39;data/count_tables/count_table_&#39;,strategy,&#39;.tsv.gz&#39;)) %&gt;% rename(genome=Genome) %&gt;% filter(genome != &quot;unmapped&quot;) %&gt;% pivot_longer(!genome, names_to = c(&quot;sample&quot;, &quot;.value&quot;), names_sep = &quot; &quot;) %&gt;% rename(&quot;read_count&quot; = &quot;Read&quot;) %&gt;% rename(&quot;covered_fraction&quot; = &quot;Covered&quot;) %&gt;% rename(&quot;MAG_length&quot; = &quot;Length&quot;) %&gt;% mutate(genome = str_replace_all(genome, &quot;^&quot;, paste0(strategy, &quot;_&quot;)), genome = str_replace_all(genome, &quot;_OP_OP&quot;, &quot;_OP&quot;), genome = str_replace_all(genome, &quot;vamb_long_C.*_C&quot;, &quot;vamb_long_C&quot;)) %&gt;% left_join(bin_metadata, by = &quot;genome&quot;) %&gt;% select(genome, sample, read_count, covered_fraction, MAG_length, primary_cluster, secondary_cluster, overall_strategy, assembly, strategy) } bin_counts &lt;- purrr::map(strategies,import_count_data) %&gt;% list_rbind() 2.3 Cluster taxonomy cluster_taxonomy &lt;- read_delim(&quot;data/cluster_taxonomy.tsv&quot;) 2.4 Cluster tree cluster_tree &lt;- read_tree(str_glue(&quot;data/cluster_tree.tre&quot;)) 2.4.1 Normalise read counts Normalise read counts into genome counts. bin_counts_norm &lt;- bin_counts %&gt;% filter(secondary_cluster %in% cluster_tree$tip.label) %&gt;% mutate(read_count = read_count*150/MAG_length) 2.4.2 Filter read counts Filter out genome counts with coverage fraction under 30%. bin_counts_norm_filt &lt;- bin_counts_norm %&gt;% mutate(read_count = ifelse(covered_fraction&gt;=0.3,read_count,0)) 2.5 Cluster counts Aggregate bins into clusters. cluster_counts &lt;- bin_counts_norm_filt %&gt;% group_by(sample,overall_strategy,secondary_cluster) %&gt;% summarise(read_count=sum(read_count), .groups=&quot;drop&quot;) %&gt;% filter(!is.na(overall_strategy)) %&gt;% select(secondary_cluster,overall_strategy,read_count) 2.6 Cluster functions cluster_kegg &lt;- read_delim(&quot;data/cluster_kegg.tsv&quot;) 2.7 Color scheme AlberdiLab projects use unified color schemes developed for the Earth Hologenome Initiative, to facilitate figure interpretation. phylum_colors &lt;- cluster_taxonomy %&gt;% left_join(read_tsv(&quot;https://raw.githubusercontent.com/earthhologenome/EHI_taxonomy_colour/main/ehi_phylum_colors.tsv&quot;) %&gt;% mutate(phylum=gsub(&quot;p__&quot;,&quot;&quot;,phylum)), by=join_by(Phylum == phylum)) %&gt;% select(Phylum, colors) %&gt;% unique() %&gt;% arrange(Phylum) %&gt;% select(colors) %&gt;% pull() strategy_colors &lt;- c(single_coverage=&quot;#5254A3&quot;, multicoverage_animal=&quot;#637939&quot;, multicoverage_timepoint_all=&quot;#8CA252&quot;, multicoverage_timepoint_cage=&quot;#B5CF6B&quot;, coassembly_animal=&quot;#8C6D31&quot;, coassembly_timepoint_all=&quot;#BD9E39&quot;, coassembly_timepoint_cage=&quot;#E7BA52&quot;, multisplit_animal=&quot;#843C39&quot;, multisplit_timepoint_all=&quot;#AD494A&quot;, multisplit_timepoint_cage=&quot;#D6616B&quot;) 2.8 Wrap working objects In the last step, the objects that are needed for downstream analyses are stored in an R object. save(bin_metadata, strategies, bin_counts, bin_counts_norm, bin_counts_norm_filt, cluster_counts, cluster_taxonomy, cluster_tree, cluster_kegg, phylum_colors, strategy_colors, file = &quot;data/data.Rdata&quot;) "],["cluster-analysis.html", "3 Cluster analysis 3.1 Cluster prevalence 3.2 Cluster frequency 3.3 Cluster heatmap 3.4 Cluster abundance 3.5 MAG mapping 3.6 Read counts vs prevalence", " 3 Cluster analysis 3.1 Cluster prevalence Number of strategies each cluster was captured in. cluster_prevalence &lt;- cluster_counts %&gt;% filter(read_count&gt;0) %&gt;% group_by(secondary_cluster) %&gt;% summarise(n_strategy = n_distinct(overall_strategy)) %&gt;% arrange(desc(n_strategy)) #Arranged by phylum and cluster prevalence cluster_prevalence_phylum &lt;- cluster_prevalence %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) 3.2 Cluster frequency Number of clusters each strategy recovered. cluster_number &lt;- cluster_counts %&gt;% filter(read_count&gt;0) %&gt;% group_by(overall_strategy) %&gt;% summarise(n = n_distinct(secondary_cluster)) %&gt;% arrange(desc(n)) %&gt;% mutate(overall_strategy=factor(overall_strategy,levels=rev(overall_strategy))) cluster_number %&gt;% rename(Strategy=overall_strategy) %&gt;% select(Strategy,n) %&gt;% tt() tinytable_3w89xuuoan1q8qepx5qh .table td.tinytable_css_b8asc0h7ptedx659l2ut, .table th.tinytable_css_b8asc0h7ptedx659l2ut { border-bottom: solid 0.1em #d3d8dc; } Strategy n coassembly_timepoint_all 176 coassembly_animal 163 coassembly_timepoint_cage 160 multicoverage_animal 130 multicoverage_timepoint_all 126 single_coverage 117 multisplit_timepoint_all 115 multisplit_timepoint_cage 110 multisplit_animal 99 multicoverage_timepoint_cage 93 cluster_number %&gt;% filter(overall_strategy != &quot;cobinning_treatment&quot;) %&gt;% ggplot(aes(x = n, y = overall_strategy)) + geom_col(color=&quot;#666666&quot;) + coord_cartesian(xlim = c(90,200))+ theme_classic() + theme() + labs(x=&quot;Number of clusters&quot;, y=&quot;Strategy&quot;) 3.3 Cluster heatmap Overview of clusters per strategy. cluster_counts %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) %&gt;% mutate(Phylum=ifelse(is.na(Phylum),&quot;Bacillota_A&quot;,Phylum)) %&gt;% mutate(secondary_cluster=factor(secondary_cluster,levels=rev(cluster_tree$tip.label))) %&gt;% mutate(overall_strategy=factor(overall_strategy,levels=rev(cluster_number$overall_strategy))) %&gt;% group_by(overall_strategy, secondary_cluster) %&gt;% sample_n(1) %&gt;% ungroup() %&gt;% ggplot(aes(x = secondary_cluster, y = overall_strategy, fill=Phylum)) + geom_tile(color=&quot;#ffffff&quot;) + scale_fill_manual(values=phylum_colors) + theme_classic() + theme(axis.text.x = element_blank(), axis.title.y = element_blank()) + xlab(&quot;MAG clusters&quot;) 3.4 Cluster abundance Maximum read count maximum_read_count &lt;- cluster_counts %&gt;% group_by(secondary_cluster) %&gt;% slice_max(order_by = read_count) %&gt;% ungroup() maximum_read_count %&gt;% mutate(secondary_cluster=factor(secondary_cluster,levels=rev(cluster_tree$tip.label))) %&gt;% ggplot(aes(x = secondary_cluster, y = read_count)) + geom_col(color=&quot;#666666&quot;) + theme_classic() + theme(axis.text.x = element_blank()) + labs(x=&quot;MAG clusters&quot;, y=&quot;Number of reads&quot;) 3.5 MAG mapping Average percentage of read mapped in each strategy against the respective catalogue. mag_mapping &lt;- bin_counts %&gt;% mutate(sample = str_replace_all(sample, &quot;_subsam&quot;, &quot;&quot;), sample = str_replace_all(sample, &quot;_M&quot;, &quot;&quot;)) %&gt;% left_join(all_counts, by = join_by(sample)) %&gt;% summarise(mag_mapping = sum(read_count) / mean(total_count), .by = c(sample, overall_strategy)) %&gt;% summarise(mag_mapping = mean(mag_mapping), .by = overall_strategy) %&gt;% mutate(overall_strategy=factor(overall_strategy,levels=rev(cluster_number$overall_strategy))) mag_mapping %&gt;% filter(!is.na(overall_strategy)) %&gt;% rename(Strategy=overall_strategy,`Mapping %`=mag_mapping) %&gt;% mutate(`Mapping %`=`Mapping %`*100) %&gt;% tt() tinytable_5whm568eem61o6776ulp .table td.tinytable_css_xhlct7xnccywwfjhxiwh, .table th.tinytable_css_xhlct7xnccywwfjhxiwh { border-bottom: solid 0.1em #d3d8dc; } Strategy Mapping % single_coverage 80.25097 multicoverage_animal 79.78157 multicoverage_timepoint_cage 79.68070 multicoverage_timepoint_all 80.48701 coassembly_animal 80.91453 coassembly_timepoint_all 82.53138 coassembly_timepoint_cage 80.11122 multisplit_animal 68.33532 multisplit_timepoint_all 83.70626 multisplit_timepoint_cage 77.11125 mag_mapping %&gt;% filter(overall_strategy != &quot;cobinning_treatment&quot;) %&gt;% ggplot(aes(x = mag_mapping, y = overall_strategy)) + geom_col(color=&quot;#666666&quot;) + coord_cartesian(xlim = c(0.6,0.9))+ theme_classic() + theme() + labs(x=&quot;Mapping percentage&quot;, y=&quot;Strategy&quot;) 3.6 Read counts vs prevalence maximum_read_count %&gt;% left_join(cluster_prevalence,by=join_by(secondary_cluster==secondary_cluster)) %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) %&gt;% select(read_count, n_strategy, Phylum) %&gt;% ggplot(aes(x = read_count, y = n_strategy, group=n_strategy, color=Phylum)) + geom_boxplot(color=&quot;#999999&quot;, fill=&quot;#f4f4f4&quot;, outlier.shape = NA) + scale_y_continuous(breaks=seq(1,10,1))+ scale_color_manual(values=phylum_colors) + geom_jitter() + theme_classic() + theme() + labs(x=&quot;Sequencing depth&quot;, y=&quot;Strategy prevalence&quot;) "],["bin-quality.html", "4 Bin quality", " 4 Bin quality load(&quot;data/data.Rdata&quot;) #Generate quality biplot bin_metadata %&gt;% select(genome,completeness,contamination,overall_strategy, size) %&gt;% ggplot(aes(x=completeness,y=contamination,size=size,color=overall_strategy)) + geom_point(alpha=0.3, size=1.5) + scale_color_manual(values=strategy_colors)+ ylim(c(10,0)) + labs(y= &quot;Contamination&quot;, x = &quot;Completeness&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) bin_metadata %&gt;% select(genome,completeness,contamination,overall_strategy, size) %&gt;% ggplot(aes(x=completeness,color=overall_strategy)) + geom_density(alpha=0.3, size=1.5, linewidth=1) + scale_color_manual(values=strategy_colors)+ labs(x = &quot;Completeness&quot;, y=&quot;Density&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) bin_metadata %&gt;% select(genome,completeness,contamination,overall_strategy, size) %&gt;% ggplot(aes(x=contamination,color=overall_strategy,)) + geom_density(alpha=0.3, size=1.5, linewidth=1) + scale_color_manual(values=strategy_colors)+ labs(x = &quot;Contamination&quot;, y=&quot;Density&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) "],["alpha-diversity.html", "5 Alpha diversity", " 5 Alpha diversity load(&quot;data/data.Rdata&quot;) alpha_div &lt;- tibble() for (strategy in strategies){ bin_counts_norm_filt_strategy &lt;- bin_counts_norm_filt %&gt;% filter(overall_strategy == {{strategy}}) %&gt;% select(secondary_cluster,sample,read_count) %&gt;% pivot_wider(names_from = &quot;sample&quot;, values_from = &quot;read_count&quot;, values_fn = sum) %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;) cluster_tree_strategy &lt;- keep.tip(cluster_tree, tip=rownames(bin_counts_norm_filt_strategy)) q0n &lt;- bin_counts_norm_filt_strategy %&gt;% hilldiv(.,q=0) %&gt;% as.numeric() q1n &lt;- bin_counts_norm_filt_strategy %&gt;% hilldiv(.,q=1) %&gt;% as.numeric() q1p &lt;- bin_counts_norm_filt_strategy %&gt;% hilldiv(.,q=1,tree=cluster_tree_strategy) %&gt;% as.numeric() alpha_div_strategy &lt;- tibble(strategy=strategy,sample=colnames(bin_counts_norm_filt_strategy),richness=q0n,neutral=round(q1n,3),phylo=round(q1p,3)) alpha_div&lt;- bind_rows(alpha_div,alpha_div_strategy) } save(alpha_div,file=&quot;results/alpha_diversity.Rdata&quot;) load(&quot;results/alpha_diversity.Rdata&quot;) alpha_div %&gt;% pivot_longer(!c(strategy,sample),names_to = &quot;metric&quot;,values_to = &quot;value&quot;) %&gt;% mutate(metric=factor(metric,levels=c(&quot;richness&quot;,&quot;neutral&quot;,&quot;phylo&quot;))) %&gt;% ggplot(aes(y=strategy, x=value))+ geom_boxplot(outlier.shape = NA)+ geom_jitter(alpha=0.1)+ facet_nested(. ~ metric, scales = &quot;free&quot;) + theme_minimal() + labs(x=&quot;Diversity&quot;,y=&quot;Strategy&quot;) "],["beta-diversity.html", "6 Beta diversity", " 6 Beta diversity load(&quot;data/data.Rdata&quot;) beta_div &lt;- list() n=0 for (strategy in strategies){ n=n+1 bin_counts_norm_filt_strategy &lt;- bin_counts_norm_filt %&gt;% filter(overall_strategy == {{strategy}}) %&gt;% select(secondary_cluster,sample,read_count) %&gt;% pivot_wider(names_from = &quot;sample&quot;, values_from = &quot;read_count&quot;, values_fn = sum) %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;) cluster_tree_strategy &lt;- keep.tip(cluster_tree, tip=rownames(bin_counts_norm_filt_strategy)) beta_q0n &lt;- bin_counts_norm_filt_strategy %&gt;% hillpair(., q=0, metric=&quot;C&quot;) %&gt;% as.dist() beta_q1n &lt;- bin_counts_norm_filt_strategy %&gt;% hillpair(., q=1, metric=&quot;C&quot;) beta_q1p &lt;- bin_counts_norm_filt_strategy %&gt;% hillpair(., q=1, metric=&quot;C&quot;, tree = cluster_tree_strategy) beta_div_strategy &lt;- list(q0n=beta_q0n,q1n=beta_q1n,q1p=beta_q1p) beta_div[[n]] &lt;- beta_div_strategy } names(beta_div) &lt;- strategies save(beta_div,file=&quot;results/beta_diversity.Rdata&quot;) "],["functions.html", "7 Functions 7.1 Trait overview 7.2 Trait recovery", " 7 Functions load(&quot;data/data.Rdata&quot;) cluster_kegg_max &lt;- cluster_kegg %&gt;% filter(!is.na(secondary_cluster)) %&gt;% select(-overall_strategy) %&gt;% group_by(secondary_cluster) %&gt;% summarise(across(everything(), max, na.rm = TRUE)) 7.1 Trait overview phylum_heatmap &lt;- read_tsv(&quot;https://raw.githubusercontent.com/earthhologenome/EHI_taxonomy_colour/main/ehi_phylum_colors.tsv&quot;) %&gt;% right_join(cluster_taxonomy, by=join_by(phylum == Phylum)) %&gt;% filter(secondary_cluster %in% cluster_tree$tip.label) %&gt;% arrange(match(secondary_cluster, cluster_tree$tip.label)) %&gt;% select(secondary_cluster,phylum) %&gt;% mutate(phylum = factor(phylum, levels = unique(phylum))) %&gt;% filter(!is.na(secondary_cluster)) %&gt;% column_to_rownames(var = &quot;secondary_cluster&quot;) function_tree &lt;- keep.tip(cluster_tree, tip=rownames(phylum_heatmap)) function_table &lt;- cluster_kegg_max %&gt;% filter(secondary_cluster %in% function_tree$tip.label) %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;) # Generate basal tree function_tree &lt;- force.ultrametric(function_tree, method=&quot;extend&quot;) %&gt;% ggtree(., size = 0.3) *************************************************************** * Note: * * force.ultrametric does not include a formal method to * * ultrametricize a tree &amp; should only be used to coerce * * a phylogeny that fails is.ultrametric due to rounding -- * * not as a substitute for formal rate-smoothing methods. * *************************************************************** #Add phylum colors next to the tree tips function_tree &lt;- gheatmap(function_tree, phylum_heatmap, offset=0, width=0.1, colnames=FALSE) + scale_fill_manual(values=phylum_colors) + labs(fill=&quot;Phylum&quot;) #Reset fill scale to use a different colour profile in the heatmap function_tree &lt;- function_tree + new_scale_fill() #Add functions heatmap function_tree &lt;- gheatmap(function_tree, function_table, offset=0.5, width=3.5, colnames=FALSE) + vexpand(.08) + coord_cartesian(clip = &quot;off&quot;) + scale_fill_gradient(low = &quot;#f4f4f4&quot;, high = &quot;steelblue&quot;, na.value=&quot;white&quot;) + labs(fill=&quot;GIFT&quot;) #Reset fill scale to use a different colour profile in the heatmap function_tree &lt;- function_tree + new_scale_fill() function_tree ## Functional ordination Rtsne(X=cluster_kegg_max %&gt;% column_to_rownames(var=&quot;secondary_cluster&quot;), dims = 2, check_duplicates = FALSE)$Y %&gt;% as.data.frame() %&gt;% mutate(secondary_cluster=cluster_kegg_max$secondary_cluster) %&gt;% rename(tSNE1=V1,tSNE2=V2) %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) %&gt;% left_join(cluster_prevalence,by=&quot;secondary_cluster&quot;) %&gt;% ggplot(aes(x = tSNE1, y = tSNE2, color = Phylum, size=n_strategy))+ geom_point(shape=16, alpha=0.7) + scale_color_manual(values=phylum_colors) + theme_minimal() + labs(color=&quot;Phylum&quot;, size=&quot;Number of strategies&quot;) + guides(color = guide_legend(override.aes = list(size = 5))) 7.2 Trait recovery all_strategy_clusters &lt;- cluster_prevalence %&gt;% filter(n_strategy==10) %&gt;% pull(secondary_cluster) # Filter clusters recovered in all strategies cluster_kegg_max_filt &lt;- cluster_kegg_max %&gt;% filter(secondary_cluster %in% all_strategy_clusters) cluster_kegg_proportion_max &lt;- cluster_kegg %&gt;% filter(!is.na(secondary_cluster)) %&gt;% filter(secondary_cluster %in% all_strategy_clusters) %&gt;% group_by(secondary_cluster) %&gt;% mutate(across(where(is.numeric), ~ . / max(., na.rm = TRUE))) %&gt;% ungroup() %&gt;% rowwise() %&gt;% mutate(average = rowMeans(across(where(is.numeric)), na.rm = TRUE)) %&gt;% select(secondary_cluster,overall_strategy,average) cluster_kegg_proportion_max %&gt;% group_by(overall_strategy) %&gt;% summarise(mean=mean(average),sd=sd(average)) %&gt;% tt() tinytable_o58sxt3y8x13p9e18jel .table td.tinytable_css_1k8cenxfbt99plrn6ptt, .table th.tinytable_css_1k8cenxfbt99plrn6ptt { border-bottom: solid 0.1em #d3d8dc; } overall_strategy mean sd coassembly_cage_treatment 0.9685454 0.03462967 coassembly_longitudinal 0.9618531 0.05465040 coassembly_treatment 0.9661473 0.04396223 cobinning_cage_treatment 0.9718577 0.03001165 cobinning_longitudinal 0.9691493 0.03665104 cobinning_treatment 0.9470009 0.08477293 individual_individual_binning 0.9640316 0.03520963 multi_split_cage_treatment 0.9481957 0.05988920 multi_split_longitudinal 0.9611543 0.03345183 multi_split_treatment 0.9846165 0.02163771 cluster_kegg_proportion_max %&gt;% left_join(cluster_taxonomy,by=&quot;secondary_cluster&quot;) %&gt;% ggplot(aes(x = average, y = overall_strategy, group=overall_strategy, color=Phylum)) + geom_boxplot(color=&quot;#999999&quot;, fill=&quot;#f4f4f4&quot;, outlier.shape = NA) + scale_color_manual(values=phylum_colors[-c(1,4,6,7)]) + xlim(0.8, 1)+ geom_jitter(alpha=0.3) + theme_classic() + theme() + labs(x=&quot;Function recovery&quot;, y=&quot;Strategy&quot;) cluster_kegg_proportion_max %&gt;% mutate(secondary_cluster=factor(secondary_cluster,levels=rev(cluster_tree$tip.label[cluster_tree$tip.label %in% cluster_kegg_proportion_max$secondary_cluster]))) %&gt;% ggplot(aes(x=secondary_cluster,y=overall_strategy,fill=average))+ geom_tile()+ scale_fill_distiller(palette = &quot;YlGnBu&quot;, direction = -1) + theme_minimal() + theme(axis.text.x = element_blank(), axis.title.y = element_blank()) + labs(y=&quot;Strategy&quot;,x=&quot;Clusters&quot;,fill=&quot;Function recovery&quot;) "],["functions-1.html", "8 Functions 8.1 Load packages and import/wrangle data 8.2 Create figure A) 8.3 Create figure B) 8.4 Patch together into the final figure", " 8 Functions 8.1 Load packages and import/wrangle data # Individual/coassembly had data collected using snakemake benchmark, so load these ind &lt;- read_delim(&quot;data/runtime/ind_benchmark.tsv&quot;, col_names = c(&quot;job&quot;, &quot;time_s&quot;, &quot;strategy&quot;)) coassembly &lt;- read_delim(&quot;data/runtime/coassembly_benchmark.tsv&quot;, col_names = c(&quot;job&quot;, &quot;time_s&quot;, &quot;strategy&quot;)) # import data into tidy format files &lt;- list.files(&quot;data/runtime&quot;, &quot;*report.tsv&quot;, full.names = T) import &lt;- function(file){ read_delim(file, col_names = c(&quot;job&quot;, &quot;time_s&quot;)) %&gt;% mutate(strategy = str_replace(file, &quot;data/runtime/&quot;, &quot;&quot;), strategy = str_replace_all(strategy, &quot;_report.tsv&quot;, &quot;&quot;)) } df &lt;- purrr::map(files, import) %&gt;% bind_rows() %&gt;% # Noticed an odd bug with snakemake report, 2 jobs have time values ~-900000000.. so filter them # Also noticed an outlier job with 824084 seconds (mean is 600 for that type) filter(time_s &gt; 0 &amp; time_s &lt; 800000) %&gt;% bind_rows(., ind, coassembly) %&gt;% mutate(job = str_replace_all(job, &quot;metaWRAP_binning&quot;, &quot;binning&quot;), job = str_replace_all(job, &quot;metaWRAP_refinement&quot;, &quot;refinement&quot;)) #Subset for comparable jobs jobs &lt;- c(&quot;binning&quot;, &quot;refinement&quot;, &quot;mapping&quot;, &quot;Assembly&quot;, &quot;assembly_mapping&quot;, &quot;checkm&quot;, &quot;vamb_multisplit&quot;) #filter by jobs of interest df_filt &lt;- df %&gt;% filter(job %in% jobs) %&gt;% mutate(job = str_replace_all(job, &quot;^mapping&quot;, &quot;assembly_mapping&quot;)) %&gt;% filter(strategy != &quot;cobinning_treat&quot;) #Add ind assembly times for cobinning/multi-split time_total_s_ind &lt;- df_filt %&gt;% filter(strategy == &quot;individual&quot; &amp; job == &quot;Assembly&quot;) %&gt;% pull(time_s) df_cobinning_cage_treat &lt;- tibble( strategy = &quot;cobinning_cage_treat&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_cobinning_long &lt;- tibble( strategy = &quot;cobinning_long&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_vamb_cage_treat &lt;- tibble( strategy = &quot;vamb_cage_treat&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_vamb_treat &lt;- tibble( strategy = &quot;vamb_treat&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_vamb_long &lt;- tibble( strategy = &quot;vamb_long&quot;, job = &quot;Assembly&quot;, time_s = time_total_s_ind ) df_filt &lt;- df_filt %&gt;% bind_rows(df_cobinning_cage_treat, df_cobinning_long, df_vamb_cage_treat, df_vamb_treat, df_vamb_long) 8.2 Create figure A) #Get sum per strategy, and rename strategies for consisitency with manuscript df_sum &lt;- df_filt %&gt;% summarise(time_total_s = sum(time_s), .by = c(&quot;strategy&quot;, &quot;job&quot;)) #Plotting colours &lt;- c(&quot;#E69F00&quot;, &quot;#56B4E9&quot;, &quot;#009E73&quot;, &quot;#F0E442&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;, &quot;#000000&quot;) fig_a &lt;- df_sum %&gt;% ggplot(aes(x = strategy, y = time_total_s / 3600, fill = job)) + geom_bar(stat = &quot;identity&quot;) + theme_classic() + theme(axis.text.x = element_text(angle = 45, size = 13, hjust = 1), axis.text.y = element_text(size = 14), axis.title = element_text(size = 14, face = &quot;bold&quot;), axis.title.x = element_blank()) + scale_fill_manual(values = colours) + labs(y = &quot;Time (hours)&quot;) #Save total time summary for another figure df_sum_total &lt;- df_sum %&gt;% summarise(time_total_s = sum(time_total_s), .by = strategy) write_delim(df_sum_total, &quot;data/runtime_total.tsv&quot;) 8.3 Create figure B) #sample size and treatment variables n1 = 15 n2 = 30 n3 = 60 n4 = 120 t1 = 1 t2 = 2 t3 = 3 t4 = 4 ################################################################################ ### individual assembly df_sum_total_ind &lt;- df_sum_total %&gt;% filter(strategy == &quot;single-coverage&quot;) %&gt;% mutate(sample_size = n2, n_treatments = t1, time_total_s = time_total_s / 5) #this creates sample_size 30, with time_total_s = 150 / 5 = 30 indiv &lt;- tibble( strategy = &quot;single-coverage&quot;, sample_size = c(n1, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4), n_treatments = c(t1, t1, t1, t2, t2, t2, t2, t3, t3, t3, t3, t4, t4, t4, t4) ) %&gt;% bind_rows(., df_sum_total_ind) %&gt;% mutate(time_total_s = (df_sum_total_ind$time_total_s / 30) * sample_size * n_treatments) #note df_sum_total_ind$time_total_s / 30 = estimated time per sample. fig_b &lt;- indiv %&gt;% ggplot(aes(x = sample_size, y = time_total_s / 3600, group = n_treatments, colour = n_treatments)) + geom_line() + geom_point() + scale_x_continuous(breaks = c(n1, n2, n3, n4)) + theme_classic() + theme(axis.title = element_text(face = &quot;bold&quot;, size = 14), axis.text = element_text(size = 14), axis.title.x = element_blank()) + ylab(&quot;Time (hours)&quot;) + ggtitle(&quot;Single-coverage binning&quot;) ################################################################################ ### coassembly df_sum_total_coa &lt;- df_sum_total %&gt;% filter(str_detect(strategy, &quot;^coassembly&quot;)) #Pull coassembly job times coassembly_df &lt;- df_filt %&gt;% filter(str_detect(strategy, &quot;^coassembly&quot;)) #Get mean times across coassembly strategies #obviously more samples = longer assemblies, but this is just a simple estimate #note: divide by n samples in coassembly mean_coassembly_t &lt;- coassembly_df %&gt;% filter(job == &quot;Assembly&quot; &amp; strategy == &quot;coassembly_timepoint_all&quot;) %&gt;% pull(time_s) %&gt;% mean() / 30 mean_coassembly_ct &lt;- coassembly_df %&gt;% filter(job == &quot;Assembly&quot; &amp; strategy == &quot;coassembly_timepoint_cage&quot;) %&gt;% pull(time_s) %&gt;% mean() / 5 mean_coassembly = (mean_coassembly_t + mean_coassembly_ct) / 2 #Do same for other jobs mean_mapping = coassembly_df %&gt;% filter(job == &quot;assembly_mapping&quot;) %&gt;% pull(time_s) %&gt;% mean() mean_binning_t &lt;- coassembly_df %&gt;% filter(job == &quot;binning&quot; &amp; strategy == &quot;coassembly_timepoint_all&quot;) %&gt;% pull(time_s) %&gt;% mean() / 30 mean_binning_ct &lt;- coassembly_df %&gt;% filter(job == &quot;binning&quot; &amp; strategy == &quot;coassembly_timepoint_cage&quot;) %&gt;% pull(time_s) %&gt;% mean() / 5 mean_binning = (mean_binning_t + mean_binning_ct) / 2 mean_refinement_t &lt;- coassembly_df %&gt;% filter(job == &quot;refinement&quot; &amp; strategy == &quot;coassembly_timepoint_all&quot;) %&gt;% pull(time_s) %&gt;% mean() / 30 mean_refinement_ct &lt;- coassembly_df %&gt;% filter(job == &quot;refinement&quot; &amp; strategy == &quot;coassembly_timepoint_cage&quot;) %&gt;% pull(time_s) %&gt;% mean() / 5 mean_refinement = (mean_refinement_t + mean_refinement_ct) / 2 #create tibble with estimatess coa &lt;- tibble( strategy = &quot;coassembly&quot;, sample_size = c(n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4), n_treatments = c(t1, t1, t1, t1, t2, t2, t2, t2, t3, t3, t3, t3, t4, t4, t4, t4) ) %&gt;% mutate(time_total_s = (mean_coassembly * sample_size * n_treatments) + (mean_mapping * sample_size * n_treatments) + (mean_binning * sample_size * n_treatments) + (mean_refinement * sample_size * n_treatments)) fig_c &lt;- coa %&gt;% ggplot(aes(x = sample_size, y = time_total_s / 3600, group = n_treatments, colour = n_treatments)) + geom_line() + geom_point() + scale_x_continuous(breaks = c(n1, n2, n3, n4)) + theme_classic() + theme(axis.title = element_text(face = &quot;bold&quot;, size = 14), axis.text = element_text(size = 14), axis.title.y = element_blank(), axis.title.x = element_blank()) + ggtitle(&quot;Coassembly&quot;) ################################################################################ ### multi-coverage binning df_cob_cage_treat &lt;- df_filt %&gt;% filter(strategy == &quot;multi-coverage_timepoint_cage&quot;) #mean time for assembly? mean_assembly &lt;- df_cob_cage_treat %&gt;% filter(job == &quot;Assembly&quot;) %&gt;% pull(time_s) %&gt;% mean() #mean time for mapping? mean_mapping &lt;- df_cob_cage_treat %&gt;% filter(job == &quot;assembly_mapping&quot;) %&gt;% pull(time_s) %&gt;% mean() #mean time for binning? mean_binning &lt;- df_cob_cage_treat %&gt;% filter(job == &quot;binning&quot;) %&gt;% pull(time_s) %&gt;% mean() #mean time for refinement? mean_refinement &lt;- df_cob_cage_treat %&gt;% filter(job == &quot;refinement&quot;) %&gt;% pull(time_s) %&gt;% mean() cob &lt;- tibble( strategy = &quot;multi-coverage binning&quot;, sample_size = c(n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4), n_treatments = c(t1, t1, t1, t1, t2, t2, t2, t2, t3, t3, t3, t3, t4, t4, t4, t4) ) %&gt;% mutate(time_total_s = (mean_assembly * sample_size * n_treatments) + (mean_mapping * (sample_size^2) * n_treatments) + (mean_binning * sample_size * n_treatments) + (mean_refinement * sample_size * n_treatments)) #scaling is sample_size, so sample_size^2 -- e.g. 2 samples = 2 * 2 mappings, 3 samples = 3 * 3 mappings, etc. fig_d &lt;- cob %&gt;% ggplot(aes(x = sample_size, y = time_total_s / 3600, group = n_treatments, colour = n_treatments)) + geom_line() + geom_point() + scale_x_continuous(breaks = c(n1, n2, n3, n4)) + theme_classic() + theme(axis.text = element_text(size = 14), axis.title = element_text(size = 14, face = &quot;bold&quot;)) + xlab(&quot;Number of samples&quot;) + ylab(&quot;Time (hours)&quot;) + ggtitle(&quot;Multi-coverage binning&quot;) ################################################################################ ### multi-split df_sum_total_ms &lt;- df_sum_total %&gt;% filter(str_detect(strategy, &quot;^multi-split&quot;)) df_ms_cage_treat &lt;- df_filt %&gt;% filter(strategy == &quot;multi-split_timepoint_cage&quot;) #mean time for assembly? mean_assembly &lt;- df_ms_cage_treat %&gt;% filter(job == &quot;Assembly&quot;) %&gt;% pull(time_s) %&gt;% mean() #How does checkm scale? checkm_t &lt;- df_filt %&gt;% filter(job == &quot;checkm&quot; &amp; strategy == &quot;multi-split_timepoint_all&quot;) checkm_ct &lt;- df_filt %&gt;% filter(job == &quot;checkm&quot; &amp; strategy == &quot;multi-split_timepoint_cage&quot;) treat &lt;- mean(checkm_t$time_s) cage_treat &lt;- mean(checkm_ct$time_s) #per sample? treat_ps &lt;- treat / 30 cage_treat_ps &lt;- cage_treat / 5 #pretty similar, so for simplicity (and given it&#39;s a fraction of the pipeline&#39;s time, let&#39;s assume the mean of the two) mean_checkm &lt;- (treat_ps + cage_treat_ps) / 2 #How does vamb_multisplit scale? vamb_multisplit_t &lt;- df_filt %&gt;% filter(job == &quot;vamb_multisplit&quot; &amp; strategy == &quot;multi-split_timepoint_all&quot;) vamb_multisplit_ct &lt;- df_filt %&gt;% filter(job == &quot;vamb_multisplit&quot; &amp; strategy == &quot;multi-split_timepoint_cage&quot;) treat &lt;- mean(vamb_multisplit_t$time_s) cage_treat &lt;- mean(vamb_multisplit_ct$time_s) #per sample? treat_ps &lt;- treat / 30 cage_treat_ps &lt;- cage_treat / 5 #pretty similar, so for simplicity (and given it&#39;s a fraction of the pipeline&#39;s time, let&#39;s assume the mean of the two) mean_vamb_multisplit &lt;- (treat_ps + cage_treat_ps) / 2 #How does assembly_mapping scale? mapping_t &lt;- df_filt %&gt;% filter(job == &quot;assembly_mapping&quot; &amp; strategy == &quot;multi-split_timepoint_all&quot;) mapping_ct &lt;- df_filt %&gt;% filter(job == &quot;assembly_mapping&quot; &amp; strategy == &quot;multi-split_timepoint_cage&quot;) treat &lt;- mean(mapping_t$time_s) cage_treat &lt;- mean(mapping_ct$time_s) # more samples combined = larger reference = longer mapping time # Find scaling exponent (note, treat has 30 samples, cage_treat 5) alpha &lt;- log(treat / cage_treat) / log(30 / 5) # Calculate a proportionality constant (choose cage_treat arbitrarily) k &lt;- cage_treat / (5) ^ alpha # Function to estimate time for 1 sample to map to combined references estimate_time &lt;- function(n) { k * (n) ^ alpha } # Estimation for 15, 30, 60, and 120 samples combined n_values &lt;- c(15, 30, 60, 120) mapping_estimated_times &lt;- sapply(n_values, estimate_time) # Create tibble with estimates ms &lt;- tibble( strategy = &quot;multi-split&quot;, sample_size = c(n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4, n1, n2, n3, n4), n_treatments = c(t1, t1, t1, t1, t2, t2, t2, t2, t3, t3, t3, t3, t4, t4, t4, t4) ) %&gt;% mutate(time_total_s = (mean_assembly * sample_size * n_treatments) + (mapping_estimated_times * sample_size * n_treatments) + (mean_vamb_multisplit * sample_size * n_treatments) + (mean_checkm * sample_size * n_treatments)) fig_e &lt;- ms %&gt;% ggplot(aes(x = sample_size, y = time_total_s / 3600, group = n_treatments, colour = n_treatments)) + geom_line() + geom_point() + scale_x_continuous(breaks = c(n1, n2, n3, n4)) + theme_classic() + theme(axis.title = element_text(face = &quot;bold&quot;, size = 14), axis.text = element_text(size = 14), axis.title.y = element_blank(),) + xlab(&quot;Number of samples&quot;) + ggtitle(&quot;Multi-split&quot;) 8.4 Patch together into the final figure fig_a / (fig_b + fig_c + fig_d + fig_e) + plot_layout(guides = &#39;collect&#39;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
